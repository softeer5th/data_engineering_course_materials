# 데일리 리뷰

날짜: 2025년 1월 6일

# ✏️ Review

## ~~📔 강의~~

## 🧑‍💻 과제

### M2

- `update`
	- WHERE문을 명시하지 않고 update를 시키면, 모든 row의 column 값이 바뀐다는 것을 주의해야된다.
- `DELETE` & `DROP TABLE`
	- DELETE에서 Where를 명시하지 않으면 전체 record가 삭제된다. 이때, table은 남아있다.
	- 한편, DROP TABLE은 table 자체를 delete 한다.
- `SELECT TOP` & `LIMIT`


	<aside>
	💡

	DBMS

	DBMS에 따라서 지원하는 명령어가 다르기 때문에 SELECT TOP의 경우, SQLite에서는 지원하지 않는 명령어이며, 대신에 LIMIT라는 명령어를 제공한다.

	</aside>


### Aggregate Functions

- `GROUP BY`
	- group by는 어떻게 작동하길래 이러한 결과가 나오는가?
	- AS를 이용해서 column 이름을 정해준 것 같은데 이게 어떠한 효과가 있는가? 어디서 사용하는가? 지금은 그냥 print만 하면 column 이름이 상관없던데
- `cursor.execute("SELECT COUNT(*) AS [Number of records] FROM Products")`
	- 대괄호는 무슨 의미인가 → 대괄호는 “” 과 같이 공백이 들어간 문자열을 column 이름으로 사용 할 수 있게 해준다.
- `LIKE`
	- wildcard `%`
		- 0개 이상의 character에 대한 wildcard
	- wildcard `_`
		- 하나의 character에 대한 wildcard
	- `[]`와 `-`는 SQLite에서 쓰이지 않는다.
- `between`
	- 양쪽을 포함하는 값을 찾으며, 날짜나 문자열에도 사용이 가능하다.
	- 날짜 표현 방식

		```python
		cursor.execute("""
					   SELECT * FROM Orders
					   WHERE OrderDate BETWEEN '1996-07-01' AND '1996-07-31;
					""")
		```

- Alias for Tables
	- Table에 대한 alias를 통해, 이후에 SQL문의 길이를 단축시킨다.

		```python
		SELECT o.OrderID, o.OrderDate, c.CustomerName
		FROM Customers AS c, Orders AS o
		WHERE c.CustomerName='Around the Horn' AND c.CustomerID=o.CustomerID;
		```


### Join

- LEFT JOIN은 일치하는 값이 TABLE1에 없더라도 TABLE1에 있는 모든 값들을 리턴해준다.

	<img src="https://github.com/minjacho42/HMG_5th/blob/master/DailyRetrospective/srcs/25_1_6_1.png" alt="Join Image" width="200"/>


### M3

> ❓**ETL이란?**
>
>
> `Extract`, `Transform`, `Load`의 순서대로 진행되며, 우리 과제의 경우에는 Extract로 웹 페이지를 크롤링 한 이후에, 웹 페이지의 표를 추출하여 원하는 형식에 맞게 transform 한 이후, region 데이터와 합쳐서 load를 진행한다.
>

> 🤔 **생각할 거리**
>
> 1. raw 데이터의 양이 압도적으로 많다면?
>
>     raw 데이터의 양이 압도적으로 많다면, 단순히 extract한 데이터를 저장하는 것은 비효율적인 방법이 될 수 있다. (저장공간이 비효율적이고, 기본적인 Transform으로 중복 처리나 여러가지 연관성이 없는 데이터들을 제거할 수 있을 것이다.)
>
> 2. raw 데이터를 Transform 하는데 시간이 아주 오래 걸린다면?
>
>     raw 데이터를 Transform 하는데 시간이 오래 걸린다면, 미리 Transform하기 전에 데이터를 먼저 저장한 이후에 Transform 하는 것이 효율적일 수 있다. (Transform 도중에, 장애가 발생할 수 있다.)
>

→ M3에서는 웹 페이지 전체를 가져오는 것이 Extract의 시작이기에, 데이터의 양이 압도적으로 많을 가능성이 높으며, 이를 한번에 저장하는 것은 비효율적일 가능성이 높다.

또한, 이후에 raw 데이터를 transform하는 경우에는, raw 데이터에서 단순히 table tag만 가져와서, 200개 언저리의 데이터들에 대한 탐색만 하면 되기에, 양이 압도적으로 많더라도 transform하는 것에 유의미하게 큰 시간이 걸리지 않을 것이라고 생각할 수 있다.

그렇기에, Transform을 마친 데이터를 json 파일에 저장하고자 한다.

> **ETL이 여러번 이루어질 가능성은 없을까?**
>
>
> 과제에서는 두번의 ETL이 요구되는 것 같다.
>
> 1. 첫번째 ETL로 웹 페이지라는 비정형 데이터를 **extract**하여, json 형식의 country, region, gdp 3개의 column을 가진 데이터로 **transform**하여 **load** 한다.
> 2. 이후 첫번째 ETL 프로세스를 통해서 **load**된 데이터를 **extract**한다. 소수점 둘째 자리까지 자르고, 각 region 별로 상위 5개의 gdp에 대한 평균을 구하며, gdp가 100B가 넘는 나라들만 필터링하는 **transform**을 진행한다. 이렇게 transform된 데이터들을 통해서 sqlite db에 **load**한다.

# 🤔 Retrospective

## 🌟 Keep

## ⚠️ Problem

## 💡 Try
