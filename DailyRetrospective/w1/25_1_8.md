# 데일리 리뷰

날짜: 2025년 1월 8일

# ✏️ Review

## 📔 강의

## 🧑‍💻 과제

### SQL QUERY (M3)

- `WITH`
    - WITH SQL query는 query문 내부에서 임시 테이블을 만드는 역할을 해준다.
    
    ```sql
    WITH <temp_table_name> (<column1>, <column2>, ...) AS (
        <sub_query>
    )
    SELECT <columns>
    FROM <temp_table_name>;
    ```
    
- `ROW_NUMBER()`, `RANK()`, `PARTITION BY`
    - `ROW_NUMBER`와 `RANK`는 모두 `OVER ( <rule> )` 를 뒤에 붙임으로서, rule에 따라서 순위를 매겨준다. 이떄, RANK는 같은 값의 경우, 같은 순위를 매기고, ROW_NUMBER는 같은 값이더라도 다른 순위를 매기게 된다.
    - `PARTITON BY` 는 `<rule>` 안에 들어갈 수 있는데, GROUP BY와 같이 그룹을 나누어서 그룹마다 순위를 매기게 된다.
    
    ```sql
    SELECT <columns>,
           ROW_NUMBER() OVER (
               [PARTITION BY <column1>, <column2>, ...]
               ORDER BY <column1> [ASC|DESC], <column2> [ASC|DESC], ...
           ) AS row_number_column
    FROM <table_name>;
    ```
    

### RAW DATA 저장 시에 메타 데이터 같이 저장하기

- extract raw data를 저장하는 이유에는 raw data를 저장함으로서, 이후에 문제가 생기거나 이전 raw data를 기반으로 새로운 insight를 창출하고 싶을 때 유용하기 때문일 것이다. 이에, 이전에는 raw data에 갱신된 부분이 있으면, 이전 데이터의 파일명을 변경하여 단순 백업을 진행한 방식에서 새로운 방식으로 변경하였다. 새로운 방식에서는 extract한 데이터에 extract한 시간에 대한 meta data를 넣은 이후에, meta data를 제외한 raw data의 정보가 같으면 meta data만 갱신하고, 전체적으로 다르다면, 이전 old 데이터를 rename하여 백업하고, 새로운 data는 새로운 meta data와 함께 저장하도록 하였다.

### 병렬처리에 알맞은 코드로 바꾸기

- 현재 transform 함수 내에 모든 transform 관련 내용이 함수화 없이 들어가 있기 때문에, 병렬 처리에 문제가 생길 가능성이 높다. 이후에, 병렬처리를 쉽게 적용하기 위해서, transform 내부의 여러 단계를 함수화 할 필요성이 있다.
    - `etl_project_gdp.py`의 경우
        1. tr 태그가 많은 경우, 병렬처리가 필요할 수 있기에, 쉽게 나누고 합칠 수 있도록, tr list를 주면, country가 key, gdp value가 value인 dict를 return하는 함수를 작성하자.
        2. row가 많은 경우, gdp value를 numeric으로 바꾸는 과정이 오래 걸릴 수 있기에, DataFrame을 받으면 지정된 column의 값을 numeric으로 바꾸는 함수를 작성하자.
        3. row가 많은 경우, gdp dataframe에 region 정보를 merge하는 작업이 오래걸릴 수 있기에, gdp dataframe과 region 정보가 있는 dataframe을 받으면, gdp dataframe을 기준으로 region dataframe과 left join하는 코드를 작성하자.

## 기타

### 코드리뷰 (코드리뷰에서 새롭게 본 부분들)

- `StringIO`
    - cpp의 stringstream과 같은 느낌의 python 내장 라이브러리 내 클래스이다.
- `pd.read_html`
    - read_html을 통해서 웹사이트 내부에 있는 값들을 DataFrame의 배열로 변환하여 정보를 받을 수 있다.
    - 잘 활용한다면, Beautifulsoup이 없어도 web scrapping이 가능할 것이라고 추측할 수 있다.
- `import pycountry_convert`
    - pycountry_convert를 통해서 region data를 위한 csv가 없더라도, 국가와 region 짝을 찾을 수 있다.
- [`pd.DataFrame.melt`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html#pandas-dataframe-melt)
    - melt는 id_vars와 value_vars를 받게 되며, id_vars를 기준으로 해당 row에 value_vars의 variable name과 value 정보를 가지는 DataFrame을 만들게 된다.

# 🤔 Retrospective

## ⚠️ Problem

- 어떠한 기준을 가지고 코드리뷰를 진행할 것인지 정하지 않고 코드리뷰를 시작하여, 초반에 체계적이지 못한 리뷰가 진행되었다.
    
    → 이후의 코드리뷰에서는 중요한 지표들을 기준으로 코드를 리뷰할 수 있도록 하자.
    

## 🌟 Keep

- 팀원들과의 코드 리뷰를 통해서, 불필요한 코드를 찾아낼 수 있었고, 이후에 해당 부분을 개선하였다.
    
    → 코드리뷰에서 피드백을 적극적으로 수용함으로서 좀 더 발전된 코드가 작성가능 할 것이다. 이후에도 계속해서 열린 마음으로 피드백을 수용할 것!
    
- 깃허브 이슈를 통한 코드리뷰 관리로, 코드리뷰 관련 추가적인 기록을 남길 수 있었다.
    
    → 계속해서 기록을 남기려고 노력하고 기록을 남김으로서 좀 더 성장할 수 있도록 하자.
    

## 💡 Try

- 이후 공부했던 것들을 다시 찾아보기 편하게 하기 위해, README에 index 부분을 추가하자.