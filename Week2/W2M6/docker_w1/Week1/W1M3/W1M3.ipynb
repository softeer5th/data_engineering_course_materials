{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W1M3 - ETL 프로세스 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 목표\n",
    "웹사이트에서 데이터를 가져와서 요구사항에 맞게 가공하는 ETL 파이프라인을 만듭니다.\n",
    "- Web Scraping에 대한 이해\n",
    "- Pandas DataFrame에 대한 이해\n",
    "- ETL Process에 대한 이해\n",
    "- Database & SQL 기초\n",
    "\n",
    "\n",
    "\n",
    "#### 사전지식\n",
    "##### 시나리오\n",
    "- 당신은 해외로 사업을 확장하고자 하는 기업에서 Data Engineer로 일하고 있습니다. 경영진에서 GDP가 높은 국가들을 대상으로 사업성을 평가하려고 합니다.\n",
    "- 이 자료는 앞으로 경영진에서 지속적으로 요구할 것으로 생각되기 때문에 자동화된 스크립트를 만들어야 합니다.\n",
    "\n",
    "##### 기능요구사항\n",
    "- IMF에서 제공하는 국가별 GDP를 구하세요. (https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29)\n",
    "- 국가별 GDP를 확인할 수 있는 테이블을 만드세요.\n",
    "- 해당 테이블에는 GDP가 높은 국가들이 먼저 나와야 합니다.\n",
    "- GDP의 단위는 1B USD이어야 하고 소수점 2자리까지만 표시해 주세요.\n",
    "- IMF에서 매년 2회 이 자료를 제공하기 때문에 정보가 갱신되더라도 해당 코드를 재사용해서 정보를 얻을 수 있어야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "#### 화면 출력\n",
    "- GDP가 100B USD이상이 되는 국가만을 구해서 화면에 출력해야 합니다.\n",
    "- 각 Region별로 top5 국가의 GDP 평균을 구해서 화면에 출력해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "#### 프로그래밍 요구사항\n",
    "##### 라이브러리 사용\n",
    "- web scaping은 BeautifulSoup4 라이브러리를 사용하세요.\n",
    "- 데이터 처리를 위해서 pandas 라이브러리를 사용하세요.\n",
    "- 로그 기록 시에 datetime 라이브러리를 사용하세요.\n",
    "\n",
    "##### 코드 가독성\n",
    "- 코드 가독성을 높이기 위해 1) 주석을 사용해서 설명을 추가하고 2) 함수를 만들어서 가독성과 재사용성을 높이세요.\n",
    "\n",
    "##### 화일 포맷, 데이터베이스 이름 등\n",
    "- 추출 (Extract)한 정보는 'Countries_by_GDP.json'라는 이름의 JSON 화일 포맷으로 저장해야 합니다.\n",
    "- 필요한 모든 작업을 수행하는 'etl_project_gdp.py' 코드를 작성하세요.\n",
    "\n",
    "##### ETL 프로세스\n",
    "- ETL 프로세스에 따라 코드를 작성하고 각 단계의 시작과 끝을 로그에 기록하세요.\n",
    "- 이 모든 처리 과정은 'etl_project_log.txt'라는 로그 화일에 기록되어야 합니다. (로그 화일은 매번 다시 생성하는 것이 아니라 기존 화일에 append되어야 합니다.)\n",
    "- log는 \"time, log\" 형식으로 기록하세요. 시간은 'Year-Monthname-Day-Hour-Minute-Second' 포맷에 맞게 표시하세요.\n",
    "\n",
    "\n",
    "\n",
    "#### 팀 활동 요구사항\n",
    "- wikipeida 페이지가 아닌, IMF 홈페이지에서 직접 데이터를 가져오는 방법은 없을까요? 어떻게 하면 될까요?\n",
    "- 만약 데이터가 갱신되면 과거의 데이터는 어떻게 되어야 할까요? 과거의 데이터를 조회하는 게 필요하다면 ETL 프로세스를 어떻게 변경해야 할까요?\n",
    "\n",
    "\n",
    "\n",
    "#### 추가 요구 사항\n",
    "##### 코드를 수정해서 아래 요구사항을 구현하세요.\n",
    "- 추출한 데이터를 데이터베이스에 저장하세요. 'Countries_by_GDP'라는 테이블명으로 'World_Economies.db'라는 데이터 베이스에 저장되어야 합니다. 해당 테이블은 'Country', 'GDP_USD_billion'라는 어트리뷰트를 반드시 가져야 합니다.\n",
    "    - 데이터베이스는 sqlite3 라이브러리를 사용해서 만드세요.\n",
    "- 필요한 모든 작업을 수행하는 'etl_project_gdp_with_sql.py' 코드를 작성하세요.\n",
    "\n",
    "##### 화면 출력\n",
    "- SQL Query를 사용해야 합니다.\n",
    "- GDP가 100B USD이상이 되는 국가만을 구해서 화면에 출력해야 합니다.\n",
    "- 각 Region별로 top5 국가의 GDP 평균을 구해서 화면에 출력해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 팀 활동 요구사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wikipeida 페이지가 아닌, IMF 홈페이지에서 직접 데이터를 가져오는 방법은 없을까요? 어떻게 하면 될까요?\n",
    "\n",
    "\n",
    "> - IMF 홈페이지에서 가져오기 방법 : https://www.imf.org/en/Publications/WEO/weo-database/2024/October/weo-report?c=512,914,612,171,614,311,213,911,314,193,122,912,313,419,513,316,913,124,339,638,514,218,963,616,223,516,918,748,618,624,522,622,156,626,628,228,924,233,632,636,634,238,662,960,423,935,128,611,321,243,248,469,253,642,643,939,734,644,819,172,132,646,648,915,134,652,174,328,258,656,654,336,263,268,532,944,176,534,536,429,433,178,436,136,343,158,439,916,664,826,542,967,443,917,544,941,446,666,668,672,946,137,546,674,676,548,556,678,181,867,682,684,273,868,921,948,943,686,688,518,728,836,558,138,196,278,692,694,962,142,449,564,565,283,853,288,293,566,964,182,359,453,968,922,714,862,135,716,456,722,942,718,724,576,936,961,813,726,199,733,184,524,361,362,364,732,366,144,146,463,528,923,738,578,537,742,866,369,744,186,925,869,746,926,466,112,111,298,927,846,299,582,487,474,754,698,&s=NGDPD,&sy=2022&ey=2029&ssm=0&scsm=1&scc=0&ssd=1&ssc=0&sic=0&sort=country&ds=.&br=1\n",
    "> - 덤프파일이나 API 사용하기  \n",
    ">       - IMF는 데이터 액세스를 위한 API를 제공한다. API를 사용하면 공식적인 방식으로 데이터를 가져올 수 있다.  \n",
    ">       - 그러므로 최신 데이터와 대규모 데이터를 자동으로 관리할 수 있을 듯 하다\n",
    "\n",
    "\n",
    "####  만약 데이터가 갱신되면 과거의 데이터는 어떻게 되어야 할까요? 과거의 데이터를 조회하는 게 필요하다면 ETL 프로세스를 어떻게 변경해야 할까요?\n",
    "> 과거의 데이터는 칼럼에 연도별로 추가해보는 방법  \n",
    "    - 데이터가 누적될수록 열(column)이 너무 많아지고, 테이블 관리가 어려워질 수 있음  \n",
    "    - 그래도 직관적이라 좋은 것 같다  \n",
    "> row가 많지 않으니까 칼럼을 쌓는 방법도 있는 듯  \n",
    "    - 추가 데이터가 생겨도 테이블 구조를 변경하지 않으므로 좋은 듯하나  \n",
    "    - 데이터 중복이 발생할 가능성도 있을 듯 하다  \n",
    "> 데이터를 데이터베이스에 적재하면서, 과거 데이터를 별도로 보관하는 방법? 혹은 유효기간을 두는 방법 등을 생각해 볼 수 있을 듯 하다\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종 완성 코드 (etl_project_gdp_with_sql.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GDP of top 5 countries by region (excluding None):\n",
      "+---------------+-------------------+\n",
      "|    Region     |   Avg_Top5_GDP    |\n",
      "+---------------+-------------------+\n",
      "|    Africa     |      256.134      |\n",
      "|     Asia      | 6255.969999999999 |\n",
      "|    Europe     |     3318.112      |\n",
      "| North America |      6946.5       |\n",
      "|    Oceania    | 734.8399999999999 |\n",
      "| South America |      797.566      |\n",
      "+---------------+-------------------+\n",
      "Countries with GDP >= 100B USD:\n",
      "+----------------------+-----------------+\n",
      "|       Country        | GDP_USD_billion |\n",
      "+----------------------+-----------------+\n",
      "|        World         |    115494.31    |\n",
      "|    United States     |    30337.16     |\n",
      "|        China         |    19534.89     |\n",
      "|       Germany        |     4921.56     |\n",
      "|        Japan         |     4389.33     |\n",
      "|        India         |     4271.92     |\n",
      "|    United Kingdom    |     3730.26     |\n",
      "|        France        |     3283.43     |\n",
      "|        Italy         |     2459.6      |\n",
      "|        Canada        |     2330.31     |\n",
      "|        Brazil        |     2307.16     |\n",
      "|        Russia        |     2195.71     |\n",
      "|     South Korea      |     1947.13     |\n",
      "|      Australia       |     1881.14     |\n",
      "|        Spain         |     1827.58     |\n",
      "|        Mexico        |     1817.82     |\n",
      "|      Indonesia       |     1492.62     |\n",
      "|        Turkey        |     1455.41     |\n",
      "|     Netherlands      |     1272.96     |\n",
      "|     Saudi Arabia     |     1136.58     |\n",
      "|     Switzerland      |      999.6      |\n",
      "|        Poland        |     915.45      |\n",
      "|        Taiwan        |     814.44      |\n",
      "|       Belgium        |     689.36      |\n",
      "|        Sweden        |     638.78      |\n",
      "|      Argentina       |      604.2      |\n",
      "|       Ireland        |     587.22      |\n",
      "| United Arab Emirates |     568.57      |\n",
      "|      Singapore       |     561.72      |\n",
      "|       Austria        |     559.22      |\n",
      "|        Israel        |      550.9      |\n",
      "|       Thailand       |     545.34      |\n",
      "|        Norway        |     503.47      |\n",
      "|       Malaysia       |     488.25      |\n",
      "|     Philippines      |     471.52      |\n",
      "|       Vietnam        |      468.4      |\n",
      "|         Iran         |     463.75      |\n",
      "|      Bangladesh      |     455.86      |\n",
      "|    Czech Republic    |     452.23      |\n",
      "|       Denmark        |     431.23      |\n",
      "|      Hong Kong       |     422.06      |\n",
      "|       Colombia       |     419.33      |\n",
      "|     South Africa     |     418.05      |\n",
      "|       Romania        |      406.2      |\n",
      "|        Egypt         |     380.04      |\n",
      "|       Pakistan       |      374.6      |\n",
      "|        Chile         |     362.24      |\n",
      "|       Finland        |     319.99      |\n",
      "|       Portugal       |     319.93      |\n",
      "|       Hungary        |     312.62      |\n",
      "|      Kazakhstan      |     306.63      |\n",
      "|         Peru         |      294.9      |\n",
      "|         Iraq         |     270.87      |\n",
      "|        Greece        |     265.17      |\n",
      "|       Algeria        |     264.27      |\n",
      "|     New Zealand      |     262.92      |\n",
      "|        Qatar         |     240.22      |\n",
      "|       Ethiopia       |     230.03      |\n",
      "|       Nigeria        |     199.72      |\n",
      "|       Ukraine        |     189.83      |\n",
      "|       Morocco        |      168.6      |\n",
      "|        Kuwait        |     161.95      |\n",
      "|       Slovakia       |     142.62      |\n",
      "|  Dominican Republic  |     126.24      |\n",
      "|      Uzbekistan      |     125.51      |\n",
      "|       Bulgaria       |     123.42      |\n",
      "|       Ecuador        |     121.42      |\n",
      "|     Puerto Rico      |     120.97      |\n",
      "|        Kenya         |     116.32      |\n",
      "|        Angola        |     113.29      |\n",
      "|      Guatemala       |     112.37      |\n",
      "|         Oman         |     110.99      |\n",
      "|      Venezuela       |     106.33      |\n",
      "+----------------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/d9knhb6x6nj7rd817gqlcsr00000gn/T/ipykernel_16942/1809515041.py:63: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]  # 위키피디아에서 제공하는 표를 Pandas로 읽고 객체를 문자열로 변환\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "# 로그 기록 시작 함수\n",
    "def log_started():\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "        log_file.write(f\"New execution at {timestamp}\")\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp} - {level} - {message}\\n\")\n",
    "\n",
    "# 설정 파일 읽기\n",
    "def load_config(config_path='config.ini'):\n",
    "    if not os.path.exists(config_path):\n",
    "        log_message(f\"Configuration file '{config_path}' not found.\", level=\"ERROR\")\n",
    "        raise FileNotFoundError((f\"Configuration file '{config_path}' not found.\"))\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    if 'DEFAULT' not in config or 'URL' not in config['DEFAULT'] or 'TABLE_CLASS' not in config['DEFAULT']:\n",
    "        log_message(\"Invalid or missing configuration values in 'config.ini'.\", level=\"ERROR\")\n",
    "        raise ValueError(\"Invalid or missing configuration values in 'config.ini'.\")\n",
    "    \n",
    "    return config['DEFAULT']['URL'], config['DEFAULT']['TABLE_CLASS']\n",
    "    \n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "def extract_gdp_data(url, table_class):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status() # HTTP 응답 상태 코드를 확인. 200번대가 아닌 경우(예: 404, 500 등) HTTPError 예외를 발생시킴\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': table_class})\n",
    "        \n",
    "        if table is None:\n",
    "            log_message(\"No table found with the specified class.\", level=\"ERROR\")\n",
    "            raise ValueError(\"Failed to locate the GDP table on the webpage.\")\n",
    "            \n",
    "        df = pd.read_html(str(table))[0]  # 위키피디아에서 제공하는 표를 Pandas로 읽고 객체를 문자열로 변환\n",
    "        \n",
    "        df = df.iloc[:, [0, 1, 2]]  # 필요한 칼럼만 선택 (모든 행과 0, 1, 2번째 열을 선택)\n",
    "        df.columns = ['Country', 'GDP (Nominal)', 'Year']\n",
    "        \n",
    "        df = df.dropna(subset=['Country', 'GDP (Nominal)']) # NaN 데이터 제거 \n",
    "        df['GDP (B USD)'] = ( # GDP 값 정리 및 변환\n",
    "            df['GDP (Nominal)']\n",
    "            .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "            .replace('', '0')  # 빈 문자열을 '0'으로 대체\n",
    "            .astype(float)  # float으로 변환\n",
    "            / 1e3  # 단위를 B USD로 변환\n",
    "        )\n",
    "        df['Year'] = df['Year'].str.replace(r'\\[.*?\\]', '', regex=True) # 각주 제거 (sup 이런 게 자꾸 따라와서..)\n",
    "        df = df[['Country', 'GDP (B USD)', 'Year']]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data extraction: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transmission\")\n",
    "    try:\n",
    "        df = df.sort_values(by='GDP (B USD)', ascending=False)  # 정렬\n",
    "        df['GDP (B USD)'] = df['GDP (B USD)'].round(2)  # 소수점 2자리로 반올림\n",
    "        \n",
    "        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "            region_data = json.load(region_file)\n",
    "        df['Region'] = df['Country'].map(region_data)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data transformation: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def load_gdp_data(df):\n",
    "    log_message(\"Loading data into SQLite database\")\n",
    "    try:\n",
    "        # SQLite 데이터베이스에 연결\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        \n",
    "        df[['Country', 'GDP (B USD)', 'Year', 'Region']].rename(\n",
    "            columns={'GDP (B USD)': 'GDP_USD_billion'}\n",
    "        ).to_sql( # 데이터프레임 데이터를 SQL 테이블로 변환하여 데이터베이스에 저장하는 pandas 메서드입\n",
    "            'Countries_by_GDP', conn, if_exists='replace', index=False\n",
    "        )\n",
    "        \n",
    "        conn.close()\n",
    "        log_message(\"Data successfully loaded into SQLite database\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error while loading data into SQLite database: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 추가 요구사항 구현\n",
    "def display_countries_with_gdp_over_100():\n",
    "    log_message(\"Displaying countries with GDP over 100B USD\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"SELECT Country, GDP_USD_billion FROM Countries_by_GDP WHERE GDP_USD_billion >= 100\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Countries with GDP >= 100B USD:\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for countries with GDP >= 100B: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "# Region별 상위 5개 국가의 GDP 평균 계산 및 출력\n",
    "def display_region_top5_average_gdp():\n",
    "    log_message(\"Calculating average GDP of top 5 countries by region\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"\"\"\n",
    "        WITH RankedCountries AS (\n",
    "            SELECT Country, GDP_USD_billion, Region,\n",
    "                   RANK() OVER (PARTITION BY Region ORDER BY GDP_USD_billion DESC) AS Rank\n",
    "            FROM Countries_by_GDP\n",
    "            WHERE Region IS NOT NULL\n",
    "        )\n",
    "        SELECT Region, AVG(GDP_USD_billion) AS Avg_Top5_GDP\n",
    "        FROM RankedCountries\n",
    "        WHERE Rank <= 5\n",
    "        GROUP BY Region\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Average GDP of top 5 countries by region (excluding None):\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for top 5 average GDP: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def etl_process():\n",
    "    try:\n",
    "        # 시작 시간 기록\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        log_started()\n",
    "        log_message(\"ETL Process Started\")\n",
    "        \n",
    "        # 설정 로드\n",
    "        url, table_class = load_config()\n",
    "        \n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data(url, table_class)\n",
    "        \n",
    "        # Save Extracted Data\n",
    "        save_gdp_data(extracted_data)\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "        \n",
    "        # Save Transformed Data\n",
    "        save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "        \n",
    "        # Load into SQLite Database\n",
    "        load_gdp_data(transformed_data)\n",
    "\n",
    "        # Additional Analyses\n",
    "        display_region_top5_average_gdp()\n",
    "        display_countries_with_gdp_over_100()\n",
    "\n",
    "\n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "        \n",
    "        # 종료 시간 기록 및 소요 시간 계산\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # 소요 시간 로그에 기록 및 출력\n",
    "        log_message(f\"ETL Process Duration: {elapsed_time}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\", level=\"ERROR\")\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작업 과정 중 완성된 코드들\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version1\n",
    "출력해야하는 함수를 따로 만들지 않고 Transform 단계에 넣은 것 (추가 요구사항 적용 X 된 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Documents/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Country  GDP (B USD)  Year         Region\n",
      "0           World    115494.31  2025            NaN\n",
      "1   United States     30337.16  2025  North America\n",
      "2           China     19534.89  2025           Asia\n",
      "3         Germany      4921.56  2025         Europe\n",
      "4           Japan      4389.33  2025           Asia\n",
      "..            ...          ...   ...            ...\n",
      "68     Uzbekistan       112.65  2024           Asia\n",
      "69      Guatemala       112.37  2024  North America\n",
      "70           Oman       109.99  2024           Asia\n",
      "71       Bulgaria       108.42  2024         Europe\n",
      "72      Venezuela       106.33  2024  South America\n",
      "\n",
      "[73 rows x 4 columns]\n",
      "          Region  Top 5 Avg GDP (B USD)\n",
      "0         Africa             238.182000\n",
      "1           Asia            6255.970000\n",
      "2         Europe            3318.112000\n",
      "3  North America            6946.500000\n",
      "4        Oceania            1212.226667\n",
      "5  South America             791.566000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/d9knhb6x6nj7rd817gqlcsr00000gn/T/ipykernel_5634/3578079735.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df.groupby('Region')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp}, {message}\\n\")\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url):\n",
    "    log_message(\"Starting Data Extraction\")\n",
    "    # HTTP 요청 후 파싱해서 데이터 추출\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:  # 첫 번째 행은 헤더\n",
    "        cols = row.find_all('td')\n",
    "        cleaned_cols = []\n",
    "        for col in cols:\n",
    "            # 'sup' 태그 제거\n",
    "            for sup in col.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            cleaned_cols.append(col.text.strip())  # 텍스트 정리\n",
    "        if cleaned_cols:  # 빈 행 제외\n",
    "            try:\n",
    "                country = cleaned_cols[0]  # 국가명\n",
    "                gdp_raw = cleaned_cols[1]  # GDP 값 (Nominal GDP)\n",
    "                gdp_year = cleaned_cols[2]\n",
    "                gdp_cleaned = ''.join(filter(str.isdigit, gdp_raw.split('.')[0]))\n",
    "                if gdp_cleaned:  # GDP 값이 유효한 경우만 추가\n",
    "                    gdp = int(gdp_cleaned) / 1e3  # 단위를 1B USD로 변환\n",
    "                    data.append({'Country': country, 'GDP (B USD)': round(gdp, 2), 'Year': gdp_year})\n",
    "            except IndexError:\n",
    "                # 예상치 못한 데이터 구조를 무시\n",
    "                continue\n",
    "    log_message(\"Data Extraction Completed\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(data):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # GDP 순으로 정렬\n",
    "    df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "    \n",
    "    # 국가별 Region 정보 매핑\n",
    "    with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "        region_data = json.load(region_file)\n",
    "    df['Region'] = df['Country'].map(region_data)\n",
    "    \n",
    "    # GDP가 100B USD 이상인 국가만 필터링\n",
    "    filtered_df = df[df['GDP (B USD)'] >= 100]\n",
    "    print(filtered_df)\n",
    "    \n",
    "    # Region별 상위 5개 국가의 GDP 평균 계산\n",
    "    region_top5_avg = (\n",
    "        filtered_df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(region_top5_avg)\n",
    "    \n",
    "    log_message(\"Data Transformation Completed\")\n",
    "    return df\n",
    "\n",
    "# Load\n",
    "def load_gdp_data(df, output_csv_file='gdp_by_country.csv', output_json_file='Countries_by_GDP.json'):\n",
    "    log_message(\"Starting Data Loading\")\n",
    "    try:\n",
    "        # CSV 파일로 저장\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        log_message(f\"CSV file saved as {output_csv_file}\")\n",
    "        \n",
    "        # JSON 파일로 저장\n",
    "        data_as_dict = df.to_dict(orient='records')  # DataFrame을 딕셔너리 리스트로 변환\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data_as_dict, json_file, ensure_ascii=False, indent=4)\n",
    "        log_message(f\"JSON file saved as {output_json_file}\")\n",
    "        \n",
    "        log_message(\"Data Loading Completed Successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Data Loading Failed: {str(e)}\")\n",
    "        raise  # 예외를 다시 던져 ETL 프로세스에서 처리 가능\n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "    try:\n",
    "        log_message(\"ETL Process Started\")\n",
    "        # Extract\n",
    "        data = extract_gdp_data(url)\n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(data)\n",
    "        # Load\n",
    "        load_gdp_data(transformed_data)  \n",
    "        log_message(\"ETL Process End Successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version2\n",
    "변경사항: 출력해야하는 것을 함수로 따로 빼놓은 것 (추가 요구사항 적용 X 된 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with a GDP of over 100B USD\n",
      "          Country  GDP (B USD)  Year         Region\n",
      "0           World    115494.31  2025            NaN\n",
      "1   United States     30337.16  2025  North America\n",
      "2           China     19534.89  2025           Asia\n",
      "3         Germany      4921.56  2025         Europe\n",
      "4           Japan      4389.33  2025           Asia\n",
      "..            ...          ...   ...            ...\n",
      "68     Uzbekistan       112.65  2024           Asia\n",
      "69      Guatemala       112.37  2024  North America\n",
      "70           Oman       109.99  2024           Asia\n",
      "71       Bulgaria       108.42  2024         Europe\n",
      "72      Venezuela       106.33  2024  South America\n",
      "\n",
      "[73 rows x 4 columns]\n",
      "Average GDP of top 5 countries by region\n",
      "          Region  Top 5 Avg GDP (B USD)\n",
      "0         Africa                238.182\n",
      "1           Asia               6255.970\n",
      "2         Europe               3318.112\n",
      "3  North America               6946.500\n",
      "4        Oceania                734.840\n",
      "5  South America                791.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/d9knhb6x6nj7rd817gqlcsr00000gn/T/ipykernel_5634/3213919250.py:95: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('Region')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp}, {message}\\n\")\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url):\n",
    "    log_message(\"Starting Data Extraction\")\n",
    "    # HTTP 요청 후 파싱해서 데이터 추출\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:  # 첫 번째 행은 헤더이므로 제외함\n",
    "        cols = row.find_all('td')\n",
    "        cleaned_cols = []\n",
    "        for col in cols:\n",
    "            # 'sup' 태그 제거함. [n 1] 이런식으로 자꾸 떠서\n",
    "            for sup in col.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            cleaned_cols.append(col.text.strip())  # 텍스트 정리\n",
    "        if cleaned_cols:  # 빈 행 제외\n",
    "            try:\n",
    "                country = cleaned_cols[0]  # 국가명\n",
    "                gdp_raw = cleaned_cols[1]  # GDP 값 (Nominal GDP)\n",
    "                gdp_year = cleaned_cols[2]\n",
    "                gdp_cleaned = ''.join(filter(str.isdigit, gdp_raw.split('.')[0]))\n",
    "                if gdp_cleaned:  # GDP 값이 유효한 경우만 추가\n",
    "                    gdp = int(gdp_cleaned) / 1e3  # 단위를 1B USD로 변환\n",
    "                    data.append({'Country': country, 'GDP (B USD)': round(gdp, 2), 'Year': gdp_year})\n",
    "            except IndexError:\n",
    "                # 예상치 못한 데이터 구조를 무시\n",
    "                continue\n",
    "    log_message(\"Data Extraction Completed\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(data):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # GDP 순으로 정렬\n",
    "    df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "    \n",
    "    # 국가별 Region 정보 매핑\n",
    "    with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "        region_data = json.load(region_file)\n",
    "    df['Region'] = df['Country'].map(region_data)\n",
    "    \n",
    "    log_message(\"Data Transformation Completed\")\n",
    "    return df\n",
    "\n",
    "# Load\n",
    "def load_gdp_data(df, output_csv_file='gdp_by_country.csv', output_json_file='Countries_by_GDP.json'):\n",
    "    log_message(\"Starting Data Loading\")\n",
    "    try:\n",
    "        # CSV 파일로 저장\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        log_message(f\"CSV file saved as {output_csv_file}\")\n",
    "        \n",
    "        # JSON 파일로 저장\n",
    "        data_as_dict = df.to_dict(orient='records')  # DataFrame을 딕셔너리 리스트로 변환\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data_as_dict, json_file, ensure_ascii=False, indent=4)\n",
    "        log_message(f\"JSON file saved as {output_json_file}\")\n",
    "        \n",
    "        log_message(\"Data Loading Completed Successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Data Loading Failed: {str(e)}\")\n",
    "        raise  # 예외를 다시 던져 ETL 프로세스에서 처리 가능\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가만 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    \n",
    "    \n",
    " # Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    \n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "    try:\n",
    "        log_message(\"ETL Process Started\")\n",
    "        # Extract\n",
    "        data = extract_gdp_data(url)\n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(data)\n",
    "        # Load\n",
    "        load_gdp_data(transformed_data)  \n",
    "        log_message(\"ETL Process End Successfully\")\n",
    "        \n",
    "        # GDP가 100B USD 이상인 국가만 필터링\n",
    "        filtered_100USD(transformed_data)\n",
    "        # Region별 상위 5개 국가의 GDP 평균 계산\n",
    "        region_top5_calculate(transformed_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 3 \n",
    "변경한 것\n",
    "1. load_gdp_data()에서 작동하는 CSV 파일로 저장하는 것과 JSON 파일로 저장하는 것을 load_gdp_data()에서 수행하는 것이 아니라, save_gdp_data() 함수로 뺌\n",
    "2. 그리고 Extract된 데이터를 Transform하기 전에 save_gdp_data를 통해 JSON, CSV로 저장함\n",
    "3. 그러므로 현재 load_gdp_data()는 수행하는 작업이 없으므로, pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with a GDP of over 100B USD\n",
      "          Country  GDP (B USD)  Year         Region\n",
      "0           World    115494.31  2025            NaN\n",
      "1   United States     30337.16  2025  North America\n",
      "2           China     19534.89  2025           Asia\n",
      "3         Germany      4921.56  2025         Europe\n",
      "4           Japan      4389.33  2025           Asia\n",
      "..            ...          ...   ...            ...\n",
      "68     Uzbekistan       112.65  2024           Asia\n",
      "69      Guatemala       112.37  2024  North America\n",
      "70           Oman       109.99  2024           Asia\n",
      "71       Bulgaria       108.42  2024         Europe\n",
      "72      Venezuela       106.33  2024  South America\n",
      "\n",
      "[73 rows x 4 columns]\n",
      "Average GDP of top 5 countries by region\n",
      "          Region  Top 5 Avg GDP (B USD)\n",
      "0         Africa                238.182\n",
      "1           Asia               6255.970\n",
      "2         Europe               3318.112\n",
      "3  North America               6946.500\n",
      "4        Oceania                734.840\n",
      "5  South America                791.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/d9knhb6x6nj7rd817gqlcsr00000gn/T/ipykernel_5634/979319949.py:87: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('Region')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp}, {message}\\n\")\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url):\n",
    "    log_message(\"Starting Data Extraction\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cleaned_cols = []\n",
    "        for col in cols:\n",
    "            for sup in col.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            cleaned_cols.append(col.text.strip())\n",
    "        if cleaned_cols:\n",
    "            try:\n",
    "                country = cleaned_cols[0]\n",
    "                gdp_raw = cleaned_cols[1]\n",
    "                gdp_year = cleaned_cols[2]\n",
    "                gdp_cleaned = ''.join(filter(str.isdigit, gdp_raw.split('.')[0]))\n",
    "                if gdp_cleaned:\n",
    "                    gdp = int(gdp_cleaned) / 1e3\n",
    "                    data.append({'Country': country, 'GDP (B USD)': round(gdp, 2), 'Year': gdp_year})\n",
    "            except IndexError:\n",
    "                continue\n",
    "    log_message(\"Data Extraction Completed\")\n",
    "    return data\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(data):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "    \n",
    "    with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "        region_data = json.load(region_file)\n",
    "    df['Region'] = df['Country'].map(region_data)\n",
    "    \n",
    "    log_message(\"Data Transformation Completed\")\n",
    "    return df\n",
    "\n",
    "# Save\n",
    "def save_gdp_data(data, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        # CSV 파일 저장\n",
    "        pd.DataFrame(data).to_csv(output_csv_file, index=False)\n",
    "        log_message(f\"Extracted data saved as CSV: {output_csv_file}\")\n",
    "        \n",
    "        # JSON 파일 저장\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "        log_message(f\"Extracted data saved as JSON: {output_json_file}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save extracted data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load\n",
    "def load_gdp_data(*args, **kwargs):\n",
    "    # 현재 이 함수는 비활성화되어 pass 처리됩니다.\n",
    "    log_message(\"load_gdp_data() is currently not performing any operation.\")\n",
    "    pass\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "    try:\n",
    "        log_message(\"ETL Process Started\")\n",
    "        # Extract\n",
    "        data = extract_gdp_data(url)\n",
    "        \n",
    "        # Save Extracted Data\n",
    "        save_gdp_data(data)  # Transform 전에 저장\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(data)\n",
    "        \n",
    "        # Additional Analyses\n",
    "        filtered_100USD(transformed_data)\n",
    "        region_top5_calculate(transformed_data)\n",
    "        \n",
    "        # Load (현재 pass 처리됨)\n",
    "        load_gdp_data()\n",
    "        \n",
    "        log_message(\"ETL Process End Successfully\")\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 4\n",
    "변경 사항\n",
    "1. Load에 있던 저장 프로세스를 Transfrom으로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp}, {message}\\n\")\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url):\n",
    "    log_message(\"Starting Data Extraction\")\n",
    "    # HTTP 요청 후 파싱해서 데이터 추출\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:  # 첫 번째 행은 헤더이므로 제외함\n",
    "        cols = row.find_all('td')\n",
    "        cleaned_cols = []\n",
    "        for col in cols:\n",
    "            # 'sup' 태그 제거 [n 1] 이런식으로 자꾸 떠서\n",
    "            for sup in col.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            cleaned_cols.append(col.text.strip())  # 텍스트 정리\n",
    "        if cleaned_cols:  # 빈 행 제외\n",
    "            try:\n",
    "                country = cleaned_cols[0]  # 국가명\n",
    "                gdp_raw = cleaned_cols[1]  # GDP 값 (Nominal GDP)\n",
    "                gdp_year = cleaned_cols[2]\n",
    "                gdp_cleaned = ''.join(filter(str.isdigit, gdp_raw.split('.')[0]))\n",
    "                if gdp_cleaned:  # GDP 값이 유효한 경우만 추가\n",
    "                    gdp = int(gdp_cleaned) / 1e3  # 단위를 1B USD로 변환\n",
    "                    data.append({'Country': country, 'GDP (B USD)': round(gdp, 2), 'Year': gdp_year})\n",
    "            except IndexError:\n",
    "                # 예상치 못한 데이터 구조를 무시\n",
    "                continue\n",
    "    log_message(\"Data Extraction Completed\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(data, output_csv_file='gdp_by_country.csv', output_json_file='Countries_by_GDP.json'):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # GDP 순으로 정렬\n",
    "    df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "    \n",
    "    # 국가별 Region 정보 매핑\n",
    "    with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "        region_data = json.load(region_file)\n",
    "    df['Region'] = df['Country'].map(region_data)\n",
    "\n",
    "    # 데이터 저장 (CSV 및 JSON 파일)\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        log_message(f\"CSV file saved as {output_csv_file}\")\n",
    "        \n",
    "        data_as_dict = df.to_dict(orient='records')  # DataFrame을 딕셔너리 리스트로 변환\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data_as_dict, json_file, ensure_ascii=False, indent=4)\n",
    "        log_message(f\"JSON file saved as {output_json_file}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Data Transformation Failed during saving: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    log_message(\"Data Transformation Completed\")\n",
    "    return df\n",
    "\n",
    "# Load\n",
    "def load_gdp_data(df):\n",
    "    log_message(\"Data Loading step is now minimal, performing any additional post-processing if required.\")\n",
    "    # 현재는 데이터 로드 관련 추가 작업이 없다면 빈 함수로 유지 가능\n",
    "    pass\n",
    "\n",
    "# GDP가 100B USD 이상인 국가만 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    \n",
    "    \n",
    " # Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "    try:\n",
    "        log_message(\"ETL Process Started\")\n",
    "        # Extract\n",
    "        data = extract_gdp_data(url)\n",
    "        # Transform (includes saving)\n",
    "        transformed_data = transform_gdp_data(data)\n",
    "        # Load\n",
    "        load_gdp_data(transformed_data)  \n",
    "        log_message(\"ETL Process End Successfully\")\n",
    "        \n",
    "        # GDP가 100B USD 이상인 국가만 필터링\n",
    "        filtered_100USD(transformed_data)\n",
    "        # Region별 상위 5개 국가의 GDP 평균 계산\n",
    "        region_top5_calculate(transformed_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 5\n",
    "변경사항\n",
    "1. Pandas의 DataFrame을 더 활용하도록 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp}, {message}\\n\")\n",
    "\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url):\n",
    "    # BeautifulSoup로 웹 페이지 스크래핑\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    # 위키피디아에서 제공하는 표를 Pandas로 읽기\n",
    "    df = pd.read_html(str(table))[0]\n",
    "\n",
    "    # 필요한 칼럼만 선택 및 이름 변경\n",
    "    df = df.iloc[:, [0, 1, 2]]  # 첫 번째, 두 번째, 세 번째 칼럼만 선택\n",
    "    df.columns = ['Country', 'GDP (Nominal)', 'Year']  # 칼럼 이름 설정\n",
    "\n",
    "    # NaN 데이터 제거 (쿠바 등)\n",
    "    df = df.dropna(subset=['Country', 'GDP (Nominal)'])\n",
    "\n",
    "    # GDP 값 정리 및 변환\n",
    "    df['GDP (B USD)'] = (\n",
    "        df['GDP (Nominal)']\n",
    "        .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "        .replace('', '0')  # 빈 문자열을 '0'으로 대체 (쿠바 등)\n",
    "        .astype(float)  # float으로 변환\n",
    "        / 1e3  # 단위를 B USD로 변환\n",
    "    )\n",
    "\n",
    "    # 각주 제거\n",
    "    df['Year'] = (\n",
    "        df['Year'].str.replace(r'\\[.*?\\]', '', regex=True)  \n",
    "    )\n",
    "\n",
    "\n",
    "    df = df[['Country', 'GDP (B USD)', 'Year']]  # 필요한 3개 칼럼만 유지\n",
    "\n",
    "    # GDP 내림차순 정렬\n",
    "    df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "\n",
    "    # 소수점 2자리로 반올림\n",
    "    df['GDP (B USD)'] = df['GDP (B USD)'].round(2)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    \n",
    "    # 정렬 및 매핑\n",
    "    df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "    with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "        region_data = json.load(region_file)\n",
    "    df['Region'] = df['Country'].map(region_data)\n",
    "\n",
    "    log_message(\"Data Transformation Completed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        # CSV 및 JSON 저장\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Load\n",
    "def load_gdp_data(*args, **kwargs):\n",
    "    # 현재 이 함수는 비활성화되어 pass 처리됩니다.\n",
    "    log_message(\"load_gdp_data() is currently not performing any operation.\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "###정렬먼저 하는 걸로 바꿔보기 egion_top5_calculate(df):\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "    try:\n",
    "        log_message(\"ETL Process Started\")\n",
    "        \n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data(url)\n",
    "        \n",
    "        # Save Extracted Data\n",
    "        save_gdp_data(extracted_data)  # Transform 전에 저장\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "        \n",
    "        # Save Transformed Data\n",
    "        save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "        \n",
    "        # Additional Analyses\n",
    "        filtered_data = filtered_100USD(transformed_data)\n",
    "        region_top5_data = region_top5_calculate(transformed_data)\n",
    "        \n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 6\n",
    "변경사항: 구조 변경에 대해 생각해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 현재 문제점\n",
    "1. 테이블 구조에 의존적  \n",
    "    - find('table', {'class': 'wikitable'})는 HTML의 특정 클래스 이름에 의존하므로, 테이블 클래스가 변경되면 실패할 가능성이 큼  \n",
    "    - df.iloc[:, [0, 1, 2]]는 칼럼 순서에 의존하므로, 칼럼 순서가 바뀌면 잘못된 데이터를 선택할 위험이 있음.\n",
    "\n",
    "2. 데이터 유효성 검증 부족  \n",
    "    - 스크래핑한 데이터가 예상 구조를 따르지 않을 경우 오류를 유발할 수 있음.\n",
    "    - 데이터 변환 단계에서 예외 처리 부족.\n",
    "    \n",
    "3. 하드코딩된 URL  \n",
    "    - URL이 변경되면 전체 ETL 프로세스가 중단됨.\n",
    "\n",
    "\n",
    "#### 해결방안?!\n",
    "1. HTML 구조 변경 대응\n",
    "\t- CSS 선택자 활용: BeautifulSoup의 CSS 선택자를 사용하여 더 유연한 테이블 선택이 가능하도록 변경합니다.\n",
    "\t- 테이블 구조 검증: 테이블이 올바르게 선택되었는지 확인하는 코드를 추가합니다.\n",
    "\t- 칼럼 이름 기반 처리: iloc 대신 칼럼 이름으로 데이터 선택을 진행하여 칼럼 순서 변경에 대응합니다.\n",
    "\n",
    "2. 데이터 유효성 검증\n",
    "\t- 테이블의 존재 여부와 칼럼 이름 확인.\n",
    "\t- 예상 데이터 형식(숫자, 문자열 등)을 확인하고, 문제가 있는 데이터를 로그로 기록하거나 무시.\n",
    "\n",
    "3. URL 변경 대응\n",
    "\t- URL을 외부 설정 파일이나 환경 변수로 관리하여 하드코딩을 제거.\n",
    "\n",
    "4. 오류 처리 및 로깅 강화\n",
    "\t- 각 단계별 오류를 캡처하고, 원인을 로그로 기록.\n",
    "\n",
    "#### 내가 고친 것\n",
    "1. 설정 파일 관리:\n",
    "    - load_config 함수로 config.ini 파일에서 URL과 HTML 테이블 클래스 값을 로드\n",
    "\t- 설정값 누락이나 파일 부재 시 예외를 발생시켜 문제를 방지\n",
    "2. 로깅 강화:\n",
    "\t- 로그 메시지에 수준(INFO, ERROR) 추가\n",
    "\t- 주요 단계마다 로그 기록으로 문제 발생 시 추적이 쉬워짐 !\n",
    "3. 예외 처리 강화:\n",
    "\t- HTTP 요청, 데이터 추출, 변환 등 주요 단계에서 예외 처리 추가함\n",
    "4. 코드의 재사용성 향상:\n",
    "\t- URL과 테이블 클래스 정보를 외부 파일에서 관리하여 유지보수성 및 유연성 높임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "# 로그 시작 함수\n",
    "def log_started():\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "        log_file.write(f\"New Execution at {timestamp}\\n\")\n",
    "        log_file.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d %H:%M:%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp} - {level} - {message}\\n\")\n",
    "\n",
    "\n",
    "# 설정 파일 읽기\n",
    "def load_config(config_path='config.ini'):\n",
    "    if not os.path.exists(config_path):\n",
    "        log_message(f\"Configuration file '{config_path}' not found.\", level=\"ERROR\")\n",
    "        raise FileNotFoundError(f\"Configuration file '{config_path}' not found.\")\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    if 'DEFAULT' not in config or 'URL' not in config['DEFAULT'] or 'TABLE_CLASS' not in config['DEFAULT']:\n",
    "        log_message(\"Invalid or missing configuration values in 'config.ini'.\", level=\"ERROR\")\n",
    "        raise ValueError(\"Invalid or missing configuration values in 'config.ini'.\")\n",
    "    \n",
    "    return config['DEFAULT']['URL'], config['DEFAULT']['TABLE_CLASS']\n",
    "\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url, table_class):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': table_class})\n",
    "\n",
    "        if table is None:\n",
    "            log_message(\"No table found with the specified class.\", level=\"ERROR\")\n",
    "            raise ValueError(\"Failed to locate the GDP table on the webpage.\")\n",
    "\n",
    "        # 위키피디아에서 제공하는 표를 Pandas로 읽기\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        # 필요한 칼럼만 선택 및 이름 변경\n",
    "        df = df.iloc[:, [0, 1, 2]]  # 첫 번째, 두 번째, 세 번째 칼럼만 선택\n",
    "        df.columns = ['Country', 'GDP (Nominal)', 'Year']  # 칼럼 이름 설정\n",
    "\n",
    "        # NaN 데이터 제거\n",
    "        df = df.dropna(subset=['Country', 'GDP (Nominal)'])\n",
    "\n",
    "        # GDP 값 정리 및 변환\n",
    "        df['GDP (B USD)'] = (\n",
    "            df['GDP (Nominal)']\n",
    "            .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "            .replace('', '0')  # 빈 문자열을 '0'으로 대체\n",
    "            .astype(float)  # float으로 변환\n",
    "            / 1e3  # 단위를 B USD로 변환\n",
    "        )\n",
    "\n",
    "        # 각주 제거\n",
    "        df['Year'] = df['Year'].str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "\n",
    "        # 필요한 3개 칼럼만 유지\n",
    "        df = df[['Country', 'GDP (B USD)', 'Year']]\n",
    "\n",
    "        # GDP 내림차순 정렬\n",
    "        df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "\n",
    "        # 소수점 2자리로 반올림\n",
    "        df['GDP (B USD)'] = df['GDP (B USD)'].round(2)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data extraction: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    try:\n",
    "        df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "            region_data = json.load(region_file)\n",
    "        df['Region'] = df['Country'].map(region_data)\n",
    "        log_message(\"Data Transformation Completed\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data transformation: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    try:\n",
    "        log_started()\n",
    "        log_message(\"ETL Process Started\")\n",
    "\n",
    "        # 설정 로드\n",
    "        url, table_class = load_config()\n",
    "\n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data(url, table_class)\n",
    "\n",
    "        # Save Extracted Data\n",
    "        save_gdp_data(extracted_data)\n",
    "\n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "\n",
    "        # Save Transformed Data\n",
    "        save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "\n",
    "        # Additional Analyses\n",
    "        filtered_data = filtered_100USD(transformed_data)\n",
    "        region_top5_data = region_top5_calculate(transformed_data)\n",
    "\n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\", level=\"ERROR\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 7\n",
    "추가 요구사항에 맞춰 코드 및 함수 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMF에서 제공하는 국가별 GDP(https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29)에서 데이터를 가져와서 요구사항에 맞게 가공하는 ETL 파이프라인을 만든 코드입니다. 코드를 수정해서 아래 요구사항을 구현하세요.\n",
    "\n",
    "1. 추출한 데이터를 데이터베이스에 저장하세요. \n",
    "    - 'Countries_by_GDP'라는 테이블명으로 'World_Economies.db'라는 데이터 베이스에 저장되어야 합니다. \n",
    "    - 해당 테이블은 'Country', 'GDP_USD_billion'라는 어트리뷰트를 반드시 가져야 합니다.\n",
    "    - 이 과정은 현재 pass 처리된 load_gdp_data()에서 진행되어야 합니다.\n",
    "\n",
    "2. 데이터베이스는 sqlite3 라이브러리를 사용해서 만드세요.\n",
    "    - 필요한 모든 작업을 수행하는 'etl_project_gdp_with_sql.py' 코드를 작성하세요.\n",
    "\n",
    "3. 화면 출력\n",
    "    - SQL Query를 사용해야 합니다.\n",
    "    - GDP가 100B USD이상이 되는 국가만을 구해서 화면에 출력해야 합니다.\n",
    "    - 각 Region별로 top5 국가의 GDP 평균을 구해서 화면에 출력해야 합니다.\n",
    "    - 이 과정은 load_gdp_data()가 아닌 따로 함수를 밖에다 새롭게 정의해서 사용해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "# 로그 시작 함수\n",
    "def log_started():\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "        log_file.write(f\"New Execution at {timestamp}\\n\")\n",
    "        log_file.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%b-%d %H:%M:%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp} - {level} - {message}\\n\")\n",
    "\n",
    "\n",
    "# 설정 파일 읽기\n",
    "def load_config(config_path='config.ini'):\n",
    "    if not os.path.exists(config_path):\n",
    "        log_message(f\"Configuration file '{config_path}' not found.\", level=\"ERROR\")\n",
    "        raise FileNotFoundError(f\"Configuration file '{config_path}' not found.\")\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    if 'DEFAULT' not in config or 'URL' not in config['DEFAULT'] or 'TABLE_CLASS' not in config['DEFAULT']:\n",
    "        log_message(\"Invalid or missing configuration values in 'config.ini'.\", level=\"ERROR\")\n",
    "        raise ValueError(\"Invalid or missing configuration values in 'config.ini'.\")\n",
    "    \n",
    "    return config['DEFAULT']['URL'], config['DEFAULT']['TABLE_CLASS']\n",
    "\n",
    "\n",
    "# Extract\n",
    "def extract_gdp_data(url, table_class):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': table_class})\n",
    "\n",
    "        if table is None:\n",
    "            log_message(\"No table found with the specified class.\", level=\"ERROR\")\n",
    "            raise ValueError(\"Failed to locate the GDP table on the webpage.\")\n",
    "\n",
    "        # 위키피디아에서 제공하는 표를 Pandas로 읽기\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        \n",
    "        # 필요한 칼럼만 선택 및 이름 변경\n",
    "        df = df.iloc[:, [0, 1, 2]]  # 첫 번째, 두 번째, 세 번째 칼럼만 선택\n",
    "        df.columns = ['Country', 'GDP (Nominal)', 'Year']  # 칼럼 이름 설정\n",
    "        # NaN 데이터 제거\n",
    "        df = df.dropna(subset=['Country', 'GDP (Nominal)'])\n",
    "        # GDP 값 정리 및 변환\n",
    "        df['GDP (B USD)'] = (\n",
    "            df['GDP (Nominal)']\n",
    "            .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "            .replace('', '0')  # 빈 문자열을 '0'으로 대체\n",
    "            .astype(float)  # float으로 변환\n",
    "            / 1e3  # 단위를 B USD로 변환\n",
    "        )\n",
    "        # 각주 제거\n",
    "        df['Year'] = df['Year'].str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "        # 필요한 3개 칼럼만 유지\n",
    "        df = df[['Country', 'GDP (B USD)', 'Year']]\n",
    "        # GDP 내림차순 정렬\n",
    "        df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "        # 소수점 2자리로 반올림\n",
    "        df['GDP (B USD)'] = df['GDP (B USD)'].round(2)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data extraction: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transformation\")\n",
    "    try:\n",
    "        df = df.sort_values(by='GDP (B USD)', ascending=False)\n",
    "        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "            region_data = json.load(region_file)\n",
    "        df['Region'] = df['Country'].map(region_data)\n",
    "        log_message(\"Data Transformation Completed\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data transformation: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Load\n",
    "# Save to SQLite Database\n",
    "def load_gdp_data(df):\n",
    "    log_message(\"Loading data into SQLite database\")\n",
    "    try:\n",
    "        # SQLite 데이터베이스에 연결\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "\n",
    "        # 데이터를 'Countries_by_GDP' 테이블로 저장\n",
    "        # 'Country', 'GDP_USD_billion', 'Year', 'Region' 칼럼 포함\n",
    "        df[['Country', 'GDP (B USD)', 'Year', 'Region']].rename(\n",
    "            columns={'GDP (B USD)': 'GDP_USD_billion'}\n",
    "        ).to_sql(\n",
    "            'Countries_by_GDP', conn, if_exists='replace', index=False\n",
    "        )\n",
    "\n",
    "        conn.close()\n",
    "        log_message(\"Data successfully loaded into SQLite database\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error while loading data into SQLite database: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# GDP 100B USD 이상 국가 출력\n",
    "def display_countries_with_gdp_over_100():\n",
    "    log_message(\"Displaying countries with GDP over 100B USD\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"SELECT Country, GDP_USD_billion FROM Countries_by_GDP WHERE GDP_USD_billion >= 100\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        print(\"Countries with GDP >= 100B USD:\")\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for countries with GDP >= 100B: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산 및 출력\n",
    "def display_region_top5_average_gdp():\n",
    "    log_message(\"Calculating average GDP of top 5 countries by region\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"\"\"\n",
    "        WITH RankedCountries AS (\n",
    "            SELECT Country, GDP_USD_billion, Region,\n",
    "                   RANK() OVER (PARTITION BY Region ORDER BY GDP_USD_billion DESC) AS Rank\n",
    "            FROM Countries_by_GDP\n",
    "            WHERE Region IS NOT NULL  -- Region이 None인 데이터를 제외\n",
    "        )\n",
    "        SELECT Region, AVG(GDP_USD_billion) AS Avg_Top5_GDP\n",
    "        FROM RankedCountries\n",
    "        WHERE Rank <= 5\n",
    "        GROUP BY Region\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        print(\"Average GDP of top 5 countries by region (excluding None):\")\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for top 5 average GDP: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 메인 ETL 함수\n",
    "def etl_process():\n",
    "    try:\n",
    "        log_started()\n",
    "        log_message(\"ETL Process Started\")\n",
    "\n",
    "        # 설정 로드\n",
    "        url, table_class = load_config()\n",
    "\n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data(url, table_class)\n",
    "\n",
    "        # Save Extracted Data\n",
    "        save_gdp_data(extracted_data)\n",
    "\n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "\n",
    "        # Save Transformed Data\n",
    "        save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "\n",
    "        # Load into SQLite Database\n",
    "        load_gdp_data(transformed_data)\n",
    "\n",
    "        # Additional Analyses\n",
    "        display_countries_with_gdp_over_100()\n",
    "        display_region_top5_average_gdp()\n",
    "\n",
    "\n",
    "        ## 추가요구사항 전 출력과정\n",
    "        #filtered_data = filtered_100USD(transformed_data)\n",
    "        #region_top5_data = region_top5_calculate(transformed_data)\n",
    "\n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\", level=\"ERROR\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform 단계를 Chunk로 나누었을 때 어떤 결과가 나올까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dummytest.py를 통해 천만개의 row를 가진 테스트 파일을 생성한 후, chunk를 나눠서 실행시간을 실험해봄\n",
    "\n",
    "| Version   | Time         |    \n",
    "|----------|---------------|\n",
    "| Original | 1:54.75       |\n",
    "| chunk (100) |1:53.90     |\n",
    "| chunk (1000) | 1:54.458      |\n",
    "| chunk (10000) | 2:01.6798       |\n",
    "| chunk (100000) | 2:45.8840       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 더 늦어지는 결과가 나타날까?\n",
    "- 합쳐지는 과정에서 시간이 걸린 것일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Code\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "# 로그 기록 시작 함수\n",
    "def log_started():\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "        log_file.write(f\"New execution at {timestamp}\")\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp} - {level} - {message}\\n\")\n",
    "\n",
    "# 설정 파일 읽기\n",
    "def load_config(config_path='config.ini'):\n",
    "    if not os.path.exists(config_path):\n",
    "        log_message(f\"Configuration file '{config_path}' not found.\", level=\"ERROR\")\n",
    "        raise FileNotFoundError((f\"Configuration file '{config_path}' not found.\"))\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    if 'DEFAULT' not in config or 'URL' not in config['DEFAULT'] or 'TABLE_CLASS' not in config['DEFAULT']:\n",
    "        log_message(\"Invalid or missing configuration values in 'config.ini'.\", level=\"ERROR\")\n",
    "        raise ValueError(\"Invalid or missing configuration values in 'config.ini'.\")\n",
    "    \n",
    "    return config['DEFAULT']['URL'], config['DEFAULT']['TABLE_CLASS']\n",
    "    \n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "def extract_gdp_data_from_csv(file_path):\n",
    "    try:\n",
    "        log_message(f\"Reading data from CSV file: {file_path}\")\n",
    "        \n",
    "        # CSV 파일에서 데이터 읽기\n",
    "        df = pd.read_csv(file_path, names=[\"Country\", \"GDP (Nominal)\", \"Year\"], skiprows=1)\n",
    "        \n",
    "        # 필요한 열 확인 및 정리\n",
    "        required_columns = ['Country', 'GDP (Nominal)', 'Year']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            log_message(f\"Missing required columns in CSV file: {file_path}\", level=\"ERROR\")\n",
    "            raise ValueError(\"CSV file does not contain required columns: 'Country', 'GDP (Nominal)', 'Year'\")\n",
    "        \n",
    "        # NaN 데이터 제거\n",
    "        df = df.dropna(subset=['Country', 'GDP (Nominal)', 'Year'])\n",
    "        \n",
    "        # GDP 값 변환\n",
    "        df['GDP (B USD)'] = (\n",
    "            df['GDP (Nominal)']\n",
    "            .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "            .replace('', '0')  # 빈 문자열을 '0'으로 대체\n",
    "            .astype(float)  # float으로 변환\n",
    "            / 1e3  # 단위를 B USD로 변환\n",
    "        )\n",
    "        \n",
    "        # 정리된 데이터 반환\n",
    "        return df[['Country', 'GDP (B USD)', 'Year']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error reading data from CSV file: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transmission\")\n",
    "    try:\n",
    "        log_message(\"Starting Data Transformation in parallel\")\n",
    "\n",
    "        # Region 데이터를 미리 로드\n",
    "        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "            region_data = json.load(region_file)\n",
    "\n",
    "        def transform_chunk(chunk):\n",
    "            # GDP 정렬 및 반올림\n",
    "            chunk = chunk.sort_values(by='GDP (B USD)', ascending=False)\n",
    "            chunk['GDP (B USD)'] = chunk['GDP (B USD)'].round(2)\n",
    "            \n",
    "            # Region 데이터를 연결\n",
    "            chunk['Region'] = chunk['Country'].map(region_data)\n",
    "            return chunk\n",
    "\n",
    "        # 데이터프레임 분할 및 병렬 처리\n",
    "        num_partitions = 100000\n",
    "        chunks = np.array_split(df, num_partitions)\n",
    "        transformed_chunks = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(transform_chunk, chunk) for chunk in chunks]\n",
    "            for future in as_completed(futures):\n",
    "                transformed_chunks.append(future.result())\n",
    "        \n",
    "        # 결과 병합\n",
    "        transformed_data = pd.concat(transformed_chunks)\n",
    "        return transformed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data transformation: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def load_gdp_data(df):\n",
    "    log_message(\"Loading data into SQLite database\")\n",
    "    try:\n",
    "        # SQLite 데이터베이스에 연결\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        \n",
    "        df[['Country', 'GDP (B USD)', 'Year', 'Region']].rename(\n",
    "            columns={'GDP (B USD)': 'GDP_USD_billion'}\n",
    "        ).to_sql( # 데이터프레임 데이터를 SQL 테이블로 변환하여 데이터베이스에 저장하는 pandas 메서드입\n",
    "            'Countries_by_GDP', conn, if_exists='replace', index=False\n",
    "        )\n",
    "        \n",
    "        conn.close()\n",
    "        log_message(\"Data successfully loaded into SQLite database\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error while loading data into SQLite database: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 추가 요구사항 구현\n",
    "def display_countries_with_gdp_over_100():\n",
    "    log_message(\"Displaying countries with GDP over 100B USD\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"SELECT Country, GDP_USD_billion FROM Countries_by_GDP WHERE GDP_USD_billion >= 100\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Countries with GDP >= 100B USD:\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for countries with GDP >= 100B: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "# Region별 상위 5개 국가의 GDP 평균 계산 및 출력\n",
    "def display_region_top5_average_gdp():\n",
    "    log_message(\"Calculating average GDP of top 5 countries by region\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"\"\"\n",
    "        WITH RankedCountries AS (\n",
    "            SELECT Country, GDP_USD_billion, Region,\n",
    "                   RANK() OVER (PARTITION BY Region ORDER BY GDP_USD_billion DESC) AS Rank\n",
    "            FROM Countries_by_GDP\n",
    "            WHERE Region IS NOT NULL\n",
    "        )\n",
    "        SELECT Region, AVG(GDP_USD_billion) AS Avg_Top5_GDP\n",
    "        FROM RankedCountries\n",
    "        WHERE Rank <= 5\n",
    "        GROUP BY Region\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Average GDP of top 5 countries by region (excluding None):\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for top 5 average GDP: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def etl_process():\n",
    "    try:\n",
    "        # 시작 시간 기록\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        log_started()\n",
    "        log_message(\"ETL Process Started\")\n",
    "        \n",
    "        # 설정 로드\n",
    "        url, table_class = load_config()\n",
    "        \n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data_from_csv(file_path='./large_data.csv')\n",
    "        \n",
    "        # Save Extracted Data\n",
    "        #save_gdp_data(extracted_data)\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "        \n",
    "        # Save Transformed Data\n",
    "        #save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "        \n",
    "        # Load into SQLite Database\n",
    "        load_gdp_data(transformed_data)\n",
    "\n",
    "        # Additional Analyses\n",
    "        display_region_top5_average_gdp()\n",
    "        display_countries_with_gdp_over_100()\n",
    "\n",
    "\n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "        \n",
    "        # 종료 시간 기록 및 소요 시간 계산\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # 소요 시간 로그에 기록 및 출력\n",
    "        log_message(f\"ETL Process Duration: {elapsed_time}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\", level=\"ERROR\")\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "# 로그 기록 시작 함수\n",
    "def log_started():\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "        log_file.write(f\"New execution at {timestamp}\")\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp} - {level} - {message}\\n\")\n",
    "\n",
    "# 설정 파일 읽기\n",
    "def load_config(config_path='config.ini'):\n",
    "    if not os.path.exists(config_path):\n",
    "        log_message(f\"Configuration file '{config_path}' not found.\", level=\"ERROR\")\n",
    "        raise FileNotFoundError((f\"Configuration file '{config_path}' not found.\"))\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    if 'DEFAULT' not in config or 'URL' not in config['DEFAULT'] or 'TABLE_CLASS' not in config['DEFAULT']:\n",
    "        log_message(\"Invalid or missing configuration values in 'config.ini'.\", level=\"ERROR\")\n",
    "        raise ValueError(\"Invalid or missing configuration values in 'config.ini'.\")\n",
    "    \n",
    "    return config['DEFAULT']['URL'], config['DEFAULT']['TABLE_CLASS']\n",
    "    \n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "def extract_gdp_data_from_csv(file_path):\n",
    "    try:\n",
    "        log_message(f\"Reading data from CSV file: {file_path}\")\n",
    "        \n",
    "        # CSV 파일에서 데이터 읽기\n",
    "        df = pd.read_csv(file_path, names=[\"Country\", \"GDP (Nominal)\", \"Year\"], skiprows=1)\n",
    "        \n",
    "        # 필요한 열 확인 및 정리\n",
    "        required_columns = ['Country', 'GDP (Nominal)', 'Year']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            log_message(f\"Missing required columns in CSV file: {file_path}\", level=\"ERROR\")\n",
    "            raise ValueError(\"CSV file does not contain required columns: 'Country', 'GDP (Nominal)', 'Year'\")\n",
    "        \n",
    "        # NaN 데이터 제거\n",
    "        df = df.dropna(subset=['Country', 'GDP (Nominal)', 'Year'])\n",
    "        \n",
    "        # GDP 값 변환\n",
    "        df['GDP (B USD)'] = (\n",
    "            df['GDP (Nominal)']\n",
    "            .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "            .replace('', '0')  # 빈 문자열을 '0'으로 대체\n",
    "            .astype(float)  # float으로 변환\n",
    "            / 1e3  # 단위를 B USD로 변환\n",
    "        )\n",
    "        \n",
    "        # 정리된 데이터 반환\n",
    "        return df[['Country', 'GDP (B USD)', 'Year']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error reading data from CSV file: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transmission\")\n",
    "    try:\n",
    "        df = df.sort_values(by='GDP (B USD)', ascending=False)  # 정렬\n",
    "        df['GDP (B USD)'] = df['GDP (B USD)'].round(2)  # 소수점 2자리로 반올림\n",
    "        \n",
    "        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "            region_data = json.load(region_file)\n",
    "        df['Region'] = df['Country'].map(region_data)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data transformation: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def load_gdp_data(df):\n",
    "    log_message(\"Loading data into SQLite database\")\n",
    "    try:\n",
    "        # SQLite 데이터베이스에 연결\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        \n",
    "        df[['Country', 'GDP (B USD)', 'Year', 'Region']].rename(\n",
    "            columns={'GDP (B USD)': 'GDP_USD_billion'}\n",
    "        ).to_sql( # 데이터프레임 데이터를 SQL 테이블로 변환하여 데이터베이스에 저장하는 pandas 메서드입\n",
    "            'Countries_by_GDP', conn, if_exists='replace', index=False\n",
    "        )\n",
    "        \n",
    "        conn.close()\n",
    "        log_message(\"Data successfully loaded into SQLite database\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error while loading data into SQLite database: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 추가 요구사항 구현\n",
    "def display_countries_with_gdp_over_100():\n",
    "    log_message(\"Displaying countries with GDP over 100B USD\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"SELECT Country, GDP_USD_billion FROM Countries_by_GDP WHERE GDP_USD_billion >= 100\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Countries with GDP >= 100B USD:\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for countries with GDP >= 100B: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "# Region별 상위 5개 국가의 GDP 평균 계산 및 출력\n",
    "def display_region_top5_average_gdp():\n",
    "    log_message(\"Calculating average GDP of top 5 countries by region\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"\"\"\n",
    "        WITH RankedCountries AS (\n",
    "            SELECT Country, GDP_USD_billion, Region,\n",
    "                   RANK() OVER (PARTITION BY Region ORDER BY GDP_USD_billion DESC) AS Rank\n",
    "            FROM Countries_by_GDP\n",
    "            WHERE Region IS NOT NULL\n",
    "        )\n",
    "        SELECT Region, AVG(GDP_USD_billion) AS Avg_Top5_GDP\n",
    "        FROM RankedCountries\n",
    "        WHERE Rank <= 5\n",
    "        GROUP BY Region\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Average GDP of top 5 countries by region (excluding None):\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for top 5 average GDP: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def etl_process():\n",
    "    try:\n",
    "        # 시작 시간 기록\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        log_started()\n",
    "        log_message(\"ETL Process Started\")\n",
    "        \n",
    "        # 설정 로드\n",
    "        url, table_class = load_config()\n",
    "        \n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data_from_csv(file_path='./large_data.csv')\n",
    "        \n",
    "        # Save Extracted Data\n",
    "        #save_gdp_data(extracted_data)\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "        \n",
    "        # Save Transformed Data\n",
    "        #save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "        \n",
    "        # Load into SQLite Database\n",
    "        load_gdp_data(transformed_data)\n",
    "\n",
    "        # Additional Analyses\n",
    "        display_region_top5_average_gdp()\n",
    "        display_countries_with_gdp_over_100()\n",
    "\n",
    "\n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "        \n",
    "        # 종료 시간 기록 및 소요 시간 계산\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # 소요 시간 로그에 기록 및 출력\n",
    "        log_message(f\"ETL Process Duration: {elapsed_time}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\", level=\"ERROR\")\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
