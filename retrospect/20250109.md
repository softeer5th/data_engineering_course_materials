## 리뷰

### 데일리 스크럼
    주영님: 
    - 코드 및 로깅 리팩토링을 함. 질문을 잘 하고 이해를 잘 하는 법을 얘기했다.

    - 상대방과의 소통을 빈번히 하고, 모르는 것은 빠르게 물어보는 것이 좋은 것 같다.
  
    도한님: 
    - 코드 재사용성이 고민이 되었다.
    - 이전에는 라이브러리 사용법을 코파일럿이 알려주었지만, 지금은 pandas 프레임워크에 익숙해지기 위하여 직접 구현하려고 한다.

    나: 
    - 어제 마지막에 깃 관련 이슈 때문에 git reflog 등을 활용하여 기록을 보고 해결하였다.
    - 따라서 github desktop을 사용하려고 한다.
    - JSON에 timestamp를 추가하여 이전 데이터를 알아야 할 것 같다. 이유는 wiki 페이지에 데이터가 갱신되는 경우가 있기 때문이다.

    오늘 할 일: ETL Process 미션 보완하기 (JSON에 timestamp 도입 및 제일 최근 데이터만으로 가져오는 방법)



---

### Timestamp
- Zoneinfo라는 기본 라이브러리를 사용하여 UTC 시간을 가져올 수 있다
- sqlite3에서는 timestamp가 아닌 datetime을 사용한다. UTC를 같이 사용할 수 없다
- 대신, 같이 사용하기 위해서 따로 Column을 만들어야 한다

### ETL (Dano님 강의를 토한 보완)

**Extract**

예외 처리가 중요하다. 긴 Extract 과정 중에 100만 줄 중 99만줄에서 에러가 나면 다시 시작하는 것이 너무 시간과 비용이 비싸다.

Extract 내부에서도 분산 처리가 필요할 수 있다. 예를 들어, 큰 파일에 대한 처리가 Extract 과정 중 있는 경우이다.

I/O intensive한 작업인 경우가 많다.

**Transform**

여러 단계가 있을 수 있다. 중간에 에러가 나면 완전히, 혹은 일부만 롤백을 할 수 있어야 한다. 그래서 Staging Area를 사용한다. 

그렇지만 ELT라면 Load 한 이후 Transform을 하기 때문에 Staging Area의 필요성이 줄어든다.

또한 열 단위 연산이 지원되기에, 병렬처리가도 용이하다.

**Load**

Load 단계에서는 Extract 과정이 추상화되어 있어야 한다. 즉, Extract 과정의 코드를 알 필요는 없이 Extract가 잘 이루어져야 한다.

### W1 리마인드 

Data product: 데이터 자체가 가치가 있고, 다시 지속적으로 사용할 수 있어야 하며, Data, Domain(작게는 단위가 천원인 것...), Access(GUI, API, 보안) 등을 고려해야 한다.

계속되는 iteration loop를 통하여 빠르게 프로토타이핑하여 요구가 정확해야 한다. 따라서 시각화 또한 설득에 필요하기에 중요하다.

예를 들어, API가 있는데 그걸 써도 되는지 바로 물어볼 수 있어야 한다.

Spark는 Pandas API를 따르기 때문에, Pandas를 잘 다루면 Spark도 잘 다룰 수 있다.

### 코드 리팩토링

ETL을 완전히 분리하여 코드를 작성하였다.

---

### Keep:계속 유지
- 중간중간 팀원들과 서로 다른 것을 하는 부분 질문하기
- ETL 과정을 엄밀하게 구분하여 코드를 작성하기

---

### Problem: 문제가 발생한 행동
- 빠른 프로토타이핑으로 수정을 하는 방식을 하진 않은 것 같다. 빠르게 수정을 하기 위해서는 빠르게 프로토타이핑을 해야 한다.

---
### Try: 다음 번에 새롭게 시도했으면 좋을 행동
- API를 통한 호출 구현해보기