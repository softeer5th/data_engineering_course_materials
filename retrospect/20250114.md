## 리뷰
  ### 데일리 스크럼
  
  * **은태**: Multiprocessing에 대한 공부. Queue에서의 empty와 full 같은 메소드에 대한 고민 → 동시성을 다루는 프로그램에 대한 기본적인 문제. 다른 장치 lock 같은 것으로 해결할 수 있을 것이다. Queue에 get과 put은 빡세고, full이나 empty는 빡세지 않다.

      프로세스를 살려두는 법 → 세마포어 이벤트로 끝날 때 까지 block, high level api.
      큐가 파이썬을 이용한 멀티 프로세싱에 코드로 많이 쓰이는 이유를 볼 수 있었다.

  * **민제**: Multiprocessing, Queue에 대해 공부했다. 
      * Python Multiprocessing에서 프로세스를 생성하는 방식 (spawn과 fork의 차이)
          * 같은 코드라도 OS마다 내부 동작이 다르다.
      * 리소스 트래커로 프로세스의 상태를 확인해봤다. (ps로 sleep 중 실행 중인 프로세스 찍기)
        ```bash
          ps > ps_outfile
          *  5166 ttys001    0:00.05 /Library/Frameworks/Python.framework/Versions/3.13/Resources/Python.app/Contents/MacOS/Python w2m1.py
              5167 ttys001    0:00.03 /Library/Frameworks/Python.framework/Versions/3.13/Resources/Python.app/Contents/MacOS/Python -c from multiprocessing.resource_tracker import main;main(6)
              5168 ttys001    0:00.03 /Library/Frameworks/Python.framework/Versions/3.13/Resources/Python.app/Contents/MacOS/Python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=7, pipe_handle=18) --multiprocessing-fork
              5169 ttys001    0:00.03 /Library/Frameworks/Python.framework/Versions/3.13/Resources/Python.app/Contents/MacOS/Python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=7, pipe_handle=20) --multiprocessing-fork
        ```
  * **민재(중복아님)**:  multiprocessing, Queue 공부. 
      * 개인적으로 배운 것: pythonic 하다 → try-except 처리에 적극적이다. 원래 에러가 발생하지 않게 최대한 분기 처리를 했는데, 파이썬은 모듈 차원에서도 예외 처리를 적극 활용한다.
      * Queue는 배치가 아닌 스트리밍 프로세스에서 사용하는 느낌. 그래서 Provider가 다수인 것까지 고려하며 코드 작성 필요.

  * 오늘 할 일: M5 진행 (이후 M6는 따로 진행)
     - 긍, 부정 별 groupby 등 따로 분리
     - 긍, 부정 별 단어 빈도 수 체크 및 의미있는 단어 샘플링
     - 데이터 정제
     - wordcloud로 데이터 시각화 (200개까지)

  ### wordcloud library
  - Wordcloud()는 __init__ 함수에서 default로 max_words가 200.
  - process_text는 내부적으로 's(소유격), 숫자, 최소 짧은 단어, stopword brute하게 제거. 이후 unigram과 bigram으로 나누어 처리. (bigram은 미사용)
  - 또한 동사 활용형도 하나의 단어라고 인식하지 않는다 -> 처리를 더욱 정밀하기 해야 한다
  - generate_from_frequencies(): 단어 빈도수 기반으로 워드 클라우드 생성. 최댓값으로 나누는 Max scaling 진행. 이후 이미지 생성.
  - 영어만 가능하고 한글은 불가능.

 ### AWS
  - 설정에 따라 ec2 user-data를 변경해도 재적용시킬 수 있다.
  - ECR에서 public 레지스트리는 적은 용량에 한해 비용이 무료이다.
  - aws-cli에서 ECR 레지스트리로 push를 해야 한다.

 ### Docker
  - docker buildx를 사용하여 멀티 플랫폼 빌드가 가능해진다.
    ```bash
    docker buildx create --name multiarch-builder --use
    
    docker buildx inspect --bootstrap

    docker buildx build --platform linux/amd64,linux/arm64 -t ket0825/myjupyter -f missions/W2/M6/Dockerfile.
    ```
  - FROM quay.io/jupyter/base-notebook가 에러. pyzmq 설치가 안됨 -> python 3.11로 downgrade   

---

### Keep:계속 유지
  - 내부 라이브러리를 파고들며 사용할 필요가 있다.

---

### Problem: 문제가 발생한 행동
  - 나라면 wordcloud에서 generate_from_frequencies()를 사용하고, 앞에 파싱은 영어의 경우 nltk, 한글의 경우 konlpy를 사용하여 처리할 것이다.

---
### Try: 다음 번에 새롭게 시도했으면 좋을 행동
- user-data를 통한 docker 설치보다 ec2의 container image를 사용하여 docker를 설치하지 않고 사용하자
- 새로운 프로그램 등의 config를 gpt보다 검색 혹은 공식문서로 해결하자