# GDP ETL Project

The goal of this project is to implement ETL process for GDP data.

## Business Requirements
The main purpose of this project is to collect GDP data from IMF website and transform it to a format that can be used for analysis.

Common analysis use cases are as follows:
1. Filter countries by GDP
2. Top N countries by GDP
3. Group by region


### Contents
---
- [GDP ETL Project](#gdp-etl-project)
  - [Business Requirements](#business-requirements)
    - [Contents](#contents)
  - [Definition of ETL Process](#definition-of-etl-process)
    - [1. Extract](#1-extract)
    - [2. Transform](#2-transform)
    - [3. Load](#3-load)
  - [Implementation](#implementation)
    - [ETL Process](#etl-process)
    - [Modules](#modules)
      - [**`importer.py`**](#importerpy)
      - [**`exporter.py`**](#exporterpy)
      - [**`logger.py`**](#loggerpy)
    - [Utils](#utils)
      - [**`create_country_region_table.py`**](#create_country_region_tablepy)
      - [**`create_large_data_csv.py`**](#create_large_data_csvpy)
  - [Performance Experiment](#performance-experiment)
    - [Read CSV](#read-csv)
    - [Pandas DataFrame](#pandas-dataframe)
  - [Parallel/Distributed Processing](#paralleldistributed-processing)
    - [Steps](#steps)

## Definition of ETL Process

### 1. Extract
- Parse html or read csv file.
- After extraction, the data should follow the format: 
    ```json
    [
        {
            "Country": "United States",
            "GDP": "30,337,162",
            "Region": "North America"
        },
        ...
    ]
    ```

### 2. Transform
- Transform GDP value
  1. Convert GDP value string to float
  2. Convert GDP value to billion
- Sort data by GDP
- After transformation, the data should follow the format:
    ```json
    [
        {
            "Country":"United States",
            "GDP":30337.16,
            "Region":"North America"
        },
        ...
    ]
    ```

### 3. Load
- Export the data to a JSON file or sqlite database.
- For optimizing query performance, store the data in GDP order.

---

## Implementation

### ETL Process
1. **`etl_project_gdp.py`**: ETL process wiki web -> json
2. **`etl_project_gdp_with_sql.py`**: ETL process wiki web -> sqlite
3. **`etl_project_gdp_from_csv.py`**: ETL process csv -> sqlite
4. **`etl_project_gdp_parallel.py`**: ETL process with Parallel/Distributed Design

### Modules

#### **`importer.py`**
Extracts data from Wikipedia and saves it to a JSON file.  
    
Supported Data Source:
- Wikipedia
- CSV File
  
Importer Class Hierarchy:  
Seperate Interface and Implementation to support multiple data source.  
- `ImporterInterface`
  - `WebImporterInterface`
    - `WikiWebImporter`
  - `FileImporterInterface`
    - `CsvFileImporter`

---

#### **`exporter.py`**
Exports the data to a JSON file.

Supported Export Target:
- JSON File
- SQLite Database(.db file)

Exporter Class Hierarchy:
- `ExporterInterface`
  - `JsonFileExporter`
  - `SqliteExporter`

---

#### **`logger.py`**
Logs the data to a file.

Supported Log Level:
- info
- error

---

### Utils

#### **`create_country_region_table.py`**
Extracts country and region data from Wikipedia and saves it to a JSON file.

Format: `{country: region}`

#### **`create_large_data_csv.py`**
Generates a large CSV file for testing.

---
## Performance Experiment

Test Data(Generated by `create_large_data_csv.py`):
- 10M row(260MB)
- 100M row(2.6GB)

Environment:
- 32GB RAM
- CPU 10 core

---

### Read CSV

```python
# 10M: 4.51s
# 100M: 48.77s
df = pd.read_csv("large_data.csv")
```

If the file is too large to fit in memory, we should use `chunksize` parameter to read the file in chunks.

```python
chunks = pd.read_csv(
    "large_data.csv",
    dtype=schema,
    header=None,
    names=schema.keys(),
    chunksize=CHUNKSIZE,
)
df = pd.concat(chunks)
```

**Estimated Result:**

More chunks(smaller chunksize), Slower
- Overhead of creating new dataframe for each chunk
- Overhead of concatenating all chunks


**Actual Result:**

|               | chunksize 10K | chunksize 100K | chunksize 1M | none   |
| ------------- | ------------- | -------------- | ------------ | ------ |
| datasize 10M  | 4.82s         | 3.92s          | 4.3s         | 4.51s  |
| datasize 100M | 46.85s        | 40.25s         | 44.38s       | 48.77s |

Regardless of data size, chunksize 100K is the fastest.

**Why?**

Pandas buffer realloc problem?

---

### Pandas DataFrame

```python
# 10M: 4.70s
# 100M: 50.19s
df["GDP"] = df["GDP"].apply(lambda x: x.replace(",", ""))
df["GDP"] = df["GDP"].apply(lambda x: round(float(x) / 1000, 2))
```

```python
# 10M: 4.14s
# 100M: 42.46s
df["GDP"] = df["GDP"].apply(lambda x: round(float(x.replace(",", "")) / 1000, 2))
```

```python
# 10M: 3.98s
# 100M: 39.55s
df["GDP"] = (
    pd.to_numeric(df["GDP"].str.replace(",", ""), errors="coerce")
    .div(1000)
    .round(2)
)
```

```python
# 10M: 3.19s
# 100M: 32.64s
df["GDP"] = (df["GDP"].replace(",", "", regex=True).astype(float) / 1000).round(2)
```

```python
# 10M: 1.66s
# 100M: 17.05s
df["GDP"] = (df["GDP"].str.replace(",", "").astype(float) / 1000).round(2)
```

---

## Parallel/Distributed Processing

Main idea: 
- Split file and process each file in parallel
- Store data seperately by region

See detail in `etl_project_gdp_parallel.py`.

### Steps

1. Split one big file to small files  
    ex) data.csv -> data_0.csv, data_1.csv
2. Preprocess each file  
    ex) data_0.csv -> data_0_preprocessed.csv
3. Map each file to region  
    ex) data_0_preprocessed.csv -> data_0_asia.csv, data_0_europe.csv
4. Reduce by region  
    ex) data_0_asia.csv, data_1_asia.csv -> data_asia.csv
5. Sort by GDP  
    ex) data_asia.csv -> data_asia_sorted.csv
6. Load to sqlite  
    ex) data_asia_sorted.csv -> data_asia_sorted.db
7. Query by region
