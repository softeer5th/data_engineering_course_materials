## 1/8 회고

### 오늘 한 일
- 다노님 질문 대해 생각 정리 및 팀 토의
    1. 여러분이 작성한 코드는 ETL 각각의 단계가 독립적으로 구분되어 작동하나요?
        - 각각의 단계를 함수로 정의하여 어느정도 독립적으로 돌아갈 수 있게 설계했다.
        - 하지만, 지금 구조로는 함수들이 순차적으로 연결되어 있고, transform 함수가 extract 함수의 return값을 인자로 받아 동작하고,
        load 함수는 transform 함수의 return값을 인자로 받아 동작하게 되는 의존성 문제가 있다.
        - 지금 구조는 extract 과정에서 에러가 생기면 그 후 함수가 돌아가지 않는 문제가 있다.
    2. 만약 ETL에서 각각의 과정을 별도의 프로세스로 처리한다면 어떻게 코드를 변경해야 할까요?
        - E T L 각각의 단계를 별도의 스크립트로 분리한 뒤 main 스크립트에서 subprocess 모듈을 사용해서 프로세스별로 실행되게끔 구현할 수 있다.
        - extract, transform, load 과정을 각각의 .py 스크립트로 분리해서 처리한다고 했을 때,
        기존 코드처럼 리턴값을 인자로 받아 처리하기가 쉽지 않다.
        - 그렇기 때문에 각각의 과정을 별도로 저장되는 파일에 의존하게끔 설정하는 방법이 좋을 것 같다.
        - extract, transform, load 각각의 과정이 끝날 때마다 처리가 완료된 데이터를 저장소에 임시 저장해놓는 것이다.
        - 그렇다면 특정 과정이 정상적으로 실행되지 않더라도 다음 과정에서 끌어올 파일만 있으면 정상 실행될 수 있다.
    3. 지금 코드는 병렬/분산 처리를 한다면 대응 가능한 코드인가요?
        - 각 단계에 대해 모듈화를 해놨기 때문에 병렬/분산 처리를 하게 된다면 어느정도 대응이 가능한 코드이지 않을까.. 생각한다.
    4. 대용량 데이터를 처리하는 과정에서 에러가 난다면?
        - 로그 출력을 자주 해주면서 어느 부분에서 문제가 생겼는지 진단을 명확히 하는 것이 가장 중요할 것 같다. 특히 대용량 데이터를 처리할 때는 리소스 관련 이슈가 많이 생길 수 있으므로 자원 모니터링을 주기적으로 하는 것이 중요할 듯.
        - 메모리 에러라면?
            - chunk 단위로 데이터를 나누어 읽어 들여 처리하는 방법을 고려할 수 있다.
            - spark 프레임워크를 활용해 데이터를 분산 처리하는 방법을 고려할 수 있다.
        - 알고리즘 에러라면?
            - 데이터 사이즈를 줄이거나 더미 데이터로 테스트 환경을 구축해 테스트하고 에러를 해결한다.
        - 일시적인 네트워크 에러라면?
            - 코드 안에 재시도 로직을 추가해 해결할 수 있다.
    5. 웹을 크롤링해와서 데이터프레임으로 만드는 과정 자체를 Transform으로 보아야할까? 그럼 데이터프레임은 원시데이터가 아닌 것일까?
        - 당연한 말이긴 하지만 팀에서 정하기 나름일 것 같다. 중요한 것은 팀원끼리 일관된 기준을 가지고 Extract와 Transform을 명확히 구별해야 한다는 것이다.
        - 다만 내가 해당 경계에 대해 결정권이 있는 포지션이라면 틀 안에 들어가는 데이터 값들이 raw 하냐라는 기준으로 접근할 것 같다.
        - DataFrame 안에 raw data 값들을 그대로 집어넣기만 한 상태라면 비정형 데이터가 정형 데이터가 된 것 뿐이지 안에 있는 데이터값은 raw data와 다를 것이 없다.
        - 이런 경우는 extract 과정이라고 생각한다.
        - 그 후 안에 있는 데이터값을 직접 수정하고 필터링하는 작업부터를 transform이라고 정할 것이다.
    6. “추출 (Extract)한 정보는 'Countries_by_GDP.json'라는 이름의 JSON 화일 포맷으로 저장해야 합니다.” ==> 추출한 데이터를 저장했을 때 어떤 이점이 있을까요?
        - 2번 질문의 답과 비슷한 맥락인 것 같다. E T L 각 과정이 전 과정의 리턴값이 아닌 파일에 의존할 수 있기 때문에 각 과정에 대해 실행 독립성이 좀 더 확보될 수 있다는 이점이 있다.

- 2번 질문에 해당하는 내용을 코드로 구현
    ```
    /gdp_etl_project
    │
    ├── data/
    │   └── country_region_table.json
    ├── log/
    │   └── etl_project_with_sql_log.txt
    ├── results/
    │   └── Countries_by_GDP.json
    ├── sqliteDB/
    │   └── World_Economies.db
    ├── extract.py
    ├── transform.py
    ├── load.py
    ├── etl_process.py
    └── utils.py
    ````
    - 다음과 같은 구조로 ETL 각 과정을 스크립트 단위로 분리하고 etl_process.py에서 subprocess 모듈을 활용해 실행되는 방식으로 구현해보았다.

### Keep

- 미션의 요구 사항을 마무리 해서 다노님이 던져주신 질문에 오래 시간을 가지고 고민했던 것 같고, 그 시간이 데이터 처리에 대해 생각을 깊게 해볼 수 있었던 시간이 되었던 것 같아서 도움이 많이 되었다.
- 다른 task를 앞으로 진행할 때도 데이터 처리에 대한 이유, 근거에 대해 많이 생각을 해보고 접근해보면 좋을 것 같다는 생각이 들었다.

### Problem

- 병렬/분산 처리가 되는 코드를 구현해보고 싶었으나 그러지 못했다.

### Try

- 팀 회의 때 팀원들 간 다양한 시각에서의 얘기가 많이 나오는데, 글로 정리하면서 듣는다면 좀 더 회의 후 생각 정리하는데 도움이 될 것 같다.