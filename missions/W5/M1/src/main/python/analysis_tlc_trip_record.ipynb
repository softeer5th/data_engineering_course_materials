{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/03 17:12:24 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.86:52245 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "25/02/03 17:12:24 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\n",
      "java.io.IOException: Failed to connect to /192.168.1.86:52245\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /192.168.1.86:52245\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import RDD\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pyspark.pandas as ps\n",
    "import io\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "ps.options.display.max_rows = 10\n",
    "# 공식 문서: compute.max_rows\n",
    "# 1000행 이하라면 driver로 데이터를 가져와서 pandas API로 처리.\n",
    "# 1000행 이상이면 pySpark로 처리\n",
    "print(ps.options.compute.max_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.id: local-1738570418598\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.name: test\n",
      "spark.app.submitTime: 1738562721766\n",
      "spark.executor.id: driver\n",
      "spark.sql.shuffle.partitions: 100\n",
      "spark.driver.bindAddress: 127.0.0.1\n",
      "spark.executor.memory: 6g\n",
      "spark.driver.memory: 6g\n",
      "spark.rdd.compress: True\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host: 192.168.1.86\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.master: local[*]\n",
      "spark.driver.port: 54108\n",
      "spark.submit.pyFiles: \n",
      "spark.submit.deployMode: client\n",
      "spark.app.startTime: 1738570418581\n",
      "spark.ui.showConsoleProgress: true\n",
      "spark.sql.execution.arrow.pyspark.enabled: true\n",
      "spark.driver.maxResultSize: 4g\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"test\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config('spark.driver.bindAddress', '127.0.0.1') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "# 1. 모든 설정 확인\n",
    "all_configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in all_configs:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# # 3. SQL 관련 설정만 확인\n",
    "# sql_configs = spark.sql(\"SET -v\").collect()\n",
    "# for row in sql_configs:\n",
    "#     print(f\"{row['key']}: {row['value']}\")\n",
    "\n",
    "# # 4. Runtime에 설정된 값들 확인\n",
    "# runtime_conf = spark.sparkContext._conf.getAll()\n",
    "# for key, value in runtime_conf:\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame으로 읽기\n",
    "psdf = spark.read.parquet(\"materials/data_engineering_course_materials/missions/W4/tlc/yellow_tripdata_2024-02.parquet\")\n",
    "\n",
    "rdd = psdf.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2024, 2, 1, 0, 4, 45), tpep_dropoff_datetime=datetime.datetime(2024, 2, 1, 0, 19, 58), passenger_count=1, trip_distance=4.39, RatecodeID=1, store_and_fwd_flag='N', PULocationID=68, DOLocationID=236, payment_type=1, fare_amount=20.5, extra=1.0, mta_tax=0.5, tip_amount=1.28, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=26.78, congestion_surcharge=2.5, Airport_fee=0.0),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2024, 2, 1, 0, 56, 31), tpep_dropoff_datetime=datetime.datetime(2024, 2, 1, 1, 10, 53), passenger_count=1, trip_distance=7.71, RatecodeID=1, store_and_fwd_flag='N', PULocationID=48, DOLocationID=243, payment_type=1, fare_amount=31.0, extra=1.0, mta_tax=0.5, tip_amount=9.0, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=45.0, congestion_surcharge=2.5, Airport_fee=0.0),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2024, 2, 1, 0, 7, 50), tpep_dropoff_datetime=datetime.datetime(2024, 2, 1, 0, 43, 12), passenger_count=2, trip_distance=28.69, RatecodeID=2, store_and_fwd_flag='N', PULocationID=132, DOLocationID=261, payment_type=2, fare_amount=70.0, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=6.94, improvement_surcharge=1.0, total_amount=82.69, congestion_surcharge=2.5, Airport_fee=1.75)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 10\n"
     ]
    }
   ],
   "source": [
    "num_partitions = rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_date = datetime.strptime(\"2024-02-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-03-01\", \"%Y-%m-%d\")\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda x: x['VendorID'] != None) \\\n",
    "    .filter(lambda x: x['tpep_pickup_datetime'] != None) \\\n",
    "    .filter(lambda x: start_date <= x['tpep_pickup_datetime'] < end_date) \\\n",
    "    .filter(lambda x: x['tpep_dropoff_datetime'] != None) \\\n",
    "    .filter(lambda x: start_date <= x['tpep_dropoff_datetime'] < end_date) \\\n",
    "    .filter(lambda x: x['passenger_count'] != None ) \\\n",
    "    .filter(lambda x: x['passenger_count'] > 0) \\\n",
    "    .filter(lambda x: x['trip_distance'] != None ) \\\n",
    "    .filter(lambda x: x['trip_distance'] > 0) \\\n",
    "    .filter(lambda x: x['RatecodeID'] != None ) \\\n",
    "    .filter(lambda x: x['store_and_fwd_flag'] != None ) \\\n",
    "    .filter(lambda x: x['PULocationID'] != None ) \\\n",
    "    .filter(lambda x: x['DOLocationID'] != None ) \\\n",
    "    .filter(lambda x: x['payment_type'] != None ) \\\n",
    "    .filter(lambda x: x['fare_amount'] != None ) \\\n",
    "    .filter(lambda x: x['fare_amount'] > 0 ) \\\n",
    "    .filter(lambda x: x['extra'] != None ) \\\n",
    "    .filter(lambda x: x['mta_tax'] != None ) \\\n",
    "    .filter(lambda x: x['tip_amount'] != None ) \\\n",
    "    .filter(lambda x: x['tolls_amount'] != None ) \\\n",
    "    .filter(lambda x: x['improvement_surcharge'] != None ) \\\n",
    "    .filter(lambda x: x['total_amount'] != None ) \\\n",
    "    .filter(lambda x: x['total_amount'] > 0 ) \\\n",
    "    .filter(lambda x: x['congestion_surcharge'] != None ) \\\n",
    "    .filter(lambda x: x['Airport_fee'] != None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2676.checkpoint.\n: org.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext\n\tat org.apache.spark.errors.SparkCoreErrors$.checkpointDirectoryHasNotBeenSetInSparkContextError(SparkCoreErrors.scala:160)\n\tat org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1660)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfiltered_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m filtered_rdd\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m~/workspace/python/DE/.venv/lib/python3.10/site-packages/pyspark/rdd.py:598\u001b[0m, in \u001b[0;36mRDD.checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03mMark this RDD for checkpointing. It will be saved to a file inside the\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03mcheckpoint directory set with :meth:`SparkContext.setCheckpointDir` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_checkpointed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/python/DE/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/python/DE/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/workspace/python/DE/.venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2676.checkpoint.\n: org.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext\n\tat org.apache.spark.errors.SparkCoreErrors$.checkpointDirectoryHasNotBeenSetInSparkContextError(SparkCoreErrors.scala:160)\n\tat org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1660)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "filtered_rdd.cache()\n",
    "filtered_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 놓친 것이 있는지 확인\n",
    "filtered_rdd.filter(lambda x: x['RatecodeID'] < 0 ) \\\n",
    "            .filter(lambda x: x['PULocationID'] < 0 ) \\\n",
    "            .filter(lambda x: x['DOLocationID'] < 0 ) \\\n",
    "            .filter(lambda x: x['payment_type'] < 0 ) \\\n",
    "            .filter(lambda x: x['fare_amount'] < 0 ) \\\n",
    "            .filter(lambda x: x['extra'] < 0 ) \\\n",
    "            .filter(lambda x: x['mta_tax'] < 0 ) \\\n",
    "            .filter(lambda x: x['tip_amount'] < 0 ) \\\n",
    "            .filter(lambda x: x['tolls_amount'] < 0 ) \\\n",
    "            .filter(lambda x: x['improvement_surcharge'] < 0 ) \\\n",
    "            .filter(lambda x: x['total_amount'] < 0 ) \\\n",
    "            .filter(lambda x: x['congestion_surcharge'] < 0 ) \\\n",
    "            .filter(lambda x: x['Airport_fee'] < 0 ) \\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Logic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue: 74431279.74007908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue: 74431279.74007908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue: 74431279.74007908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 총 매출\n",
    "# 방법 1: map, sum 사용\n",
    "tot_revenue = filtered_rdd.map(lambda x: x['total_amount']).sum()\n",
    "print(f\"Total revenue: {tot_revenue}\")\n",
    "\n",
    "# 방법 2: reduce 사용\n",
    "tot_revenue = filtered_rdd.map(lambda x: x['total_amount']).reduce(lambda x, y: x + y)\n",
    "print(f\"Total revenue: {tot_revenue}\")\n",
    "# 방법 3: fold 사용 (초기 시작값이 있음)\n",
    "tot_revenue = filtered_rdd.map(lambda x: x['total_amount']).fold(0, lambda acc, x: acc + x)\n",
    "print(f\"Total revenue: {tot_revenue}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips: 2718651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 총 trip 수\n",
    "tot_trips = filtered_rdd.count()\n",
    "print(f\"Total trips: {tot_trips}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips: 2718651\n",
      "Total revenue: 74431279.74007908\n",
      "Average distance: 3.419221959714512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stats = filtered_rdd.aggregate(\n",
    "    (0, 0.0, 0), # 초기값 (trip 수, 총 매출, 평균 거리)\n",
    "    # combine value\n",
    "    # # 각 partition에서 수행할 연산 (map)\n",
    "    lambda acc, x: (\n",
    "        acc[0] +1, \n",
    "        acc[1] + x['total_amount'],\n",
    "        acc[2] + x['trip_distance'],\n",
    "        ),\n",
    "    # 각 partition 결과를 합치는 연산 (combine combiners)\n",
    "    lambda acc1, acc2: ( # \n",
    "        acc1[0] + acc2[0], \n",
    "        acc1[1] + acc2[1],\n",
    "        acc1[2] + acc2[2],        \n",
    "        ),     \n",
    ")\n",
    "\n",
    "print(f\"Total trips: {stats[0]}\")\n",
    "print(f\"Total revenue: {stats[1]}\")\n",
    "print(f\"Average distance: {stats[2] / stats[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01: 102070\n",
      "2024-02-02: 98021\n",
      "2024-02-03: 100990\n",
      "2024-02-04: 80983\n",
      "2024-02-05: 82904\n",
      "2024-02-06: 95328\n",
      "2024-02-07: 95906\n",
      "2024-02-08: 105306\n",
      "2024-02-09: 101060\n",
      "2024-02-10: 102715\n",
      "2024-02-11: 85507\n",
      "2024-02-12: 85609\n",
      "2024-02-13: 64354\n",
      "2024-02-14: 108629\n",
      "2024-02-15: 107069\n",
      "2024-02-16: 94776\n",
      "2024-02-17: 91075\n",
      "2024-02-18: 80718\n",
      "2024-02-19: 70098\n",
      "2024-02-20: 90266\n",
      "2024-02-21: 96718\n",
      "2024-02-22: 105507\n",
      "2024-02-23: 94591\n",
      "2024-02-24: 103426\n",
      "2024-02-25: 82841\n",
      "2024-02-26: 79291\n",
      "2024-02-27: 100885\n",
      "2024-02-28: 100272\n",
      "2024-02-29: 111736\n"
     ]
    }
   ],
   "source": [
    "# 1. map, reduceByKey, sortByKey 사용\n",
    "# number of trips per day\n",
    "daily_trips = filtered_rdd.map(\n",
    "    lambda x: (x['tpep_pickup_datetime'].date(), 1)\n",
    ").reduceByKey(lambda x, y: x + y).sortByKey()\n",
    "\n",
    "for date, count in daily_trips.collect():\n",
    "    print(f\"{date}: {count}\")\n",
    "\n",
    "# 2. groupByKey 사용 (누적하는 것이 아닌 한 번에 메모리에 올리기 때문에 메모리 부족할 수 있음)\n",
    "# 2m 30s\n",
    "# number of trips per day\n",
    "# daily_trips = filtered_rdd.keyBy(\n",
    "#     lambda x: x['tpep_pickup_datetime'].strftime('%Y-%m-%d')\n",
    "# ).groupByKey().mapValues(\n",
    "#     lambda x: len(list(x))\n",
    "# ).collect()\n",
    "\n",
    "# for date, count in daily_trips:\n",
    "#     print(f\"{date}: {count}\")\n",
    "\n",
    "# 3. Join도 가능하지만 Inner 값이기에 비효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01: 2824015.919999942\n",
      "2024-02-02: 2620733.9299999326\n",
      "2024-02-03: 2491385.499999978\n",
      "2024-02-04: 2237400.2499998766\n",
      "2024-02-05: 2316529.6799999434\n",
      "2024-02-06: 2580443.3999999897\n",
      "2024-02-07: 2696189.720000007\n",
      "2024-02-08: 2960078.48999986\n",
      "2024-02-09: 2804011.5299998904\n",
      "2024-02-10: 2608000.2499999367\n",
      "2024-02-11: 2330352.6399999503\n",
      "2024-02-12: 2400751.849999933\n",
      "2024-02-13: 1637187.279999944\n",
      "2024-02-14: 2909888.909999908\n",
      "2024-02-15: 2928848.609999918\n",
      "2024-02-16: 2679060.249999922\n",
      "2024-02-17: 2312374.7899999036\n",
      "2024-02-18: 2187241.4499999024\n",
      "2024-02-19: 2080471.0499999125\n",
      "2024-02-20: 2546112.629999959\n",
      "2024-02-21: 2635096.6199999717\n",
      "2024-02-22: 2952750.1499999287\n",
      "2024-02-23: 2614497.559999943\n",
      "2024-02-24: 2626553.7999999304\n",
      "2024-02-25: 2336178.379999912\n",
      "2024-02-26: 2398380.339999892\n",
      "2024-02-27: 2768085.449999888\n",
      "2024-02-28: 2819418.339999884\n",
      "2024-02-29: 3129240.9699998614\n"
     ]
    }
   ],
   "source": [
    "# Total revenue per day\n",
    "daily_revenue = filtered_rdd.map(\n",
    "    lambda x: (x['tpep_pickup_datetime'].date(), x['total_amount'])\n",
    ").reduceByKey(lambda x, y: x + y).sortByKey()\n",
    "\n",
    "for date, revenue in daily_revenue.collect():\n",
    "    print(f\"{date}: {revenue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2024, 2, 4), 80983, 2237400.2499998766),\n",
       " (datetime.date(2024, 2, 13), 64354, 1637187.279999944),\n",
       " (datetime.date(2024, 2, 17), 91075, 2312374.7899999036),\n",
       " (datetime.date(2024, 2, 21), 96718, 2635096.6199999717),\n",
       " (datetime.date(2024, 2, 1), 102070, 2824015.919999942),\n",
       " (datetime.date(2024, 2, 8), 105306, 2960078.48999986),\n",
       " (datetime.date(2024, 2, 11), 85507, 2330352.6399999503),\n",
       " (datetime.date(2024, 2, 14), 108629, 2909888.909999908),\n",
       " (datetime.date(2024, 2, 16), 94776, 2679060.249999922),\n",
       " (datetime.date(2024, 2, 19), 70098, 2080471.0499999125),\n",
       " (datetime.date(2024, 2, 9), 101060, 2804011.5299998904),\n",
       " (datetime.date(2024, 2, 24), 103426, 2626553.7999999304),\n",
       " (datetime.date(2024, 2, 26), 79291, 2398380.339999892),\n",
       " (datetime.date(2024, 2, 6), 95328, 2580443.3999999897),\n",
       " (datetime.date(2024, 2, 15), 107069, 2928848.609999918),\n",
       " (datetime.date(2024, 2, 28), 100272, 2819418.339999884),\n",
       " (datetime.date(2024, 2, 18), 80718, 2187241.4499999024),\n",
       " (datetime.date(2024, 2, 25), 82841, 2336178.379999912),\n",
       " (datetime.date(2024, 2, 10), 102715, 2608000.2499999367),\n",
       " (datetime.date(2024, 2, 7), 95906, 2696189.720000007),\n",
       " (datetime.date(2024, 2, 20), 90266, 2546112.629999959),\n",
       " (datetime.date(2024, 2, 22), 105507, 2952750.1499999287),\n",
       " (datetime.date(2024, 2, 27), 100885, 2768085.449999888),\n",
       " (datetime.date(2024, 2, 5), 82904, 2316529.6799999434),\n",
       " (datetime.date(2024, 2, 23), 94591, 2614497.559999943),\n",
       " (datetime.date(2024, 2, 3), 100990, 2491385.499999978),\n",
       " (datetime.date(2024, 2, 2), 98021, 2620733.9299999326),\n",
       " (datetime.date(2024, 2, 12), 85609, 2400751.849999933),\n",
       " (datetime.date(2024, 2, 29), 111736, 3129240.9699998614)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_trips_revenue = daily_trips.join(daily_revenue).map(\n",
    "    lambda x: (x[0], x[1][0], x[1][1])\n",
    ")\n",
    "daily_trips_revenue.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'materials/data_engineering_course_materials/missions/W5/results/'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# 기본적으로 partition으로 저장됨\n",
    "\n",
    "# Save results as CSV\n",
    "daily_trips_revenue = daily_trips_revenue.toDF(['date', 'count', 'revenue'])\n",
    "# 파티션 1개로 저장하기\n",
    "daily_trips_revenue.coalesce(1).write.csv(f'{base_dir}/daily_trips_revenues.csv', header=True, mode=\"overwrite\")\n",
    "\n",
    "# Save results as Parquet\n",
    "stats_df = spark.createDataFrame([stats], ['total_trips', 'total_revenue', 'avg_distance'])\n",
    "stats_df.coalesce(1).write.parquet(f'{base_dir}/stats.parquet', mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
