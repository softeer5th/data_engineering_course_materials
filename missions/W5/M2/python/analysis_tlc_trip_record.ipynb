{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import from_utc_timestamp, to_utc_timestamp\n",
    "import pyspark.pandas as ps\n",
    "import io\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xmx8g\n",
      "Picked up _JAVA_OPTIONS: -Xmx8g\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/04 10:07:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "ps.options.display.max_rows = 10\n",
    "# 공식 문서: compute.max_rows\n",
    "# 1000행 이하라면 driver로 데이터를 가져와서 pandas API로 처리.\n",
    "# 1000행 이상이면 pySpark로 처리\n",
    "print(ps.options.compute.max_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.sql.warehouse.dir: file:/Users/admin/workspace/python/DE/spark-warehouse\n",
      "spark.executor.id: driver\n",
      "spark.driver.port: 53034\n",
      "spark.app.id: local-1738631268821\n",
      "spark.rdd.compress: True\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.name: pandas-on-Spark\n",
      "spark.driver.host: 192.168.1.86\n",
      "spark.app.startTime: 1738631268317\n",
      "spark.app.submitTime: 1738631268255\n",
      "spark.master: local[*]\n",
      "spark.submit.pyFiles: \n",
      "spark.submit.deployMode: client\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.ui.showConsoleProgress: true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 10:07:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"test\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config('spark.driver.bindAddress', '127.0.0.1') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "# 1. 모든 설정 확인\n",
    "all_configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in all_configs:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# # 3. SQL 관련 설정만 확인\n",
    "# sql_configs = spark.sql(\"SET -v\").collect()\n",
    "# for row in sql_configs:\n",
    "#     print(f\"{row['key']}: {row['value']}\")\n",
    "\n",
    "# # 4. Runtime에 설정된 값들 확인\n",
    "# runtime_conf = spark.sparkContext._conf.getAll()\n",
    "# for key, value in runtime_conf:\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame으로 읽기\n",
    "psdf = spark.read.parquet(\"materials/data_engineering_course_materials/missions/W4/tlc/yellow_tripdata_2024-02.parquet\")\n",
    "psdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psdf = psdf.withColumn(\"tpep_pickup_datetime\", \n",
    "                          to_utc_timestamp(from_utc_timestamp(psdf[\"tpep_pickup_datetime\"], \"America/New_York\"), \"America/New_York\")) \\\n",
    "            .withColumn(\"tpep_dropoff_datetime\", \n",
    "                          to_utc_timestamp(from_utc_timestamp(psdf[\"tpep_dropoff_datetime\"], \"America/New_York\"), \"America/New_York\"))\n",
    "\n",
    "psdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame(psdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 3007526 entries, 0 to 3007525\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   VendorID               3007526 non-null  int32         \n",
      " 1   tpep_pickup_datetime   3007526 non-null  datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  3007526 non-null  datetime64[ns]\n",
      " 3   passenger_count        2821916 non-null  int64         \n",
      " 4   trip_distance          3007526 non-null  float64       \n",
      " 5   RatecodeID             2821916 non-null  int64         \n",
      " 6   store_and_fwd_flag     2821916 non-null  object        \n",
      " 7   PULocationID           3007526 non-null  int32         \n",
      " 8   DOLocationID           3007526 non-null  int32         \n",
      " 9   payment_type           3007526 non-null  int64         \n",
      " 10  fare_amount            3007526 non-null  float64       \n",
      " 11  extra                  3007526 non-null  float64       \n",
      " 12  mta_tax                3007526 non-null  float64       \n",
      " 13  tip_amount             3007526 non-null  float64       \n",
      " 14  tolls_amount           3007526 non-null  float64       \n",
      " 15  improvement_surcharge  3007526 non-null  float64       \n",
      " 16  total_amount           3007526 non-null  float64       \n",
      " 17  congestion_surcharge   2821916 non-null  float64       \n",
      " 18  Airport_fee            2821916 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(10), int32(3), int64(3), object(1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 10:43:15 WARN AttachDistributedSequenceExec: clean up cached RDD(62) in AttachDistributedSequenceExec(235)\n"
     ]
    }
   ],
   "source": [
    "psdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_date = datetime.strptime(\"2024-02-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-03-01\", \"%Y-%m-%d\")\n",
    "\n",
    "query = f\"\"\"\n",
    "    VendorID IS NOT NULL and\n",
    "    tpep_pickup_datetime IS NOT NULL and\n",
    "    tpep_pickup_datetime BETWEEN \"{start_date}\" and \"{end_date}\" and\n",
    "    tpep_dropoff_datetime IS NOT NULL and\n",
    "    tpep_dropoff_datetime BETWEEN \"{start_date}\" and \"{end_date}\" and\n",
    "    passenger_count IS NOT NULL and\n",
    "    passenger_count > 0 and\n",
    "    trip_distance IS NOT NULL and\n",
    "    trip_distance > 0 and\n",
    "    RatecodeID IS NOT NULL and\n",
    "    store_and_fwd_flag IS NOT NULL and\n",
    "    PULocationID IS NOT NULL and\n",
    "    DOLocationID IS NOT NULL and\n",
    "    payment_type IS NOT NULL and\n",
    "    fare_amount IS NOT NULL and\n",
    "    fare_amount > 0 and\n",
    "    extra IS NOT NULL and\n",
    "    mta_tax IS NOT NULL and\n",
    "    tip_amount IS NOT NULL and\n",
    "    tolls_amount IS NOT NULL and\n",
    "    improvement_surcharge IS NOT NULL and\n",
    "    total_amount IS NOT NULL and\n",
    "    total_amount > 0 and\n",
    "    congestion_surcharge IS NOT NULL and\n",
    "    Airport_fee IS NOT NULL\n",
    "\"\"\"\n",
    "filtered_psdf = psdf.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 2718661 entries, 0 to 2821915\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   VendorID               2718661 non-null  int32         \n",
      " 1   tpep_pickup_datetime   2718661 non-null  datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  2718661 non-null  datetime64[ns]\n",
      " 3   passenger_count        2718661 non-null  int64         \n",
      " 4   trip_distance          2718661 non-null  float64       \n",
      " 5   RatecodeID             2718661 non-null  int64         \n",
      " 6   store_and_fwd_flag     2718661 non-null  object        \n",
      " 7   PULocationID           2718661 non-null  int32         \n",
      " 8   DOLocationID           2718661 non-null  int32         \n",
      " 9   payment_type           2718661 non-null  int64         \n",
      " 10  fare_amount            2718661 non-null  float64       \n",
      " 11  extra                  2718661 non-null  float64       \n",
      " 12  mta_tax                2718661 non-null  float64       \n",
      " 13  tip_amount             2718661 non-null  float64       \n",
      " 14  tolls_amount           2718661 non-null  float64       \n",
      " 15  improvement_surcharge  2718661 non-null  float64       \n",
      " 16  total_amount           2718661 non-null  float64       \n",
      " 17  congestion_surcharge   2718661 non-null  float64       \n",
      " 18  Airport_fee            2718661 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(10), int32(3), int64(3), object(1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 14:27:17 WARN AttachDistributedSequenceExec: clean up cached RDD(133) in AttachDistributedSequenceExec(574)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_psdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>estimated_cost_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, Airport_fee, estimated_cost_amount]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 놓친 것이 있는지 확인\n",
    "query = f\"\"\"\n",
    "    RatecodeID < 0 or\n",
    "    PULocationID < 0 or\n",
    "    DOLocationID < 0 or\n",
    "    payment_type < 0 or\n",
    "    fare_amount < 0 or\n",
    "    extra < 0 or\n",
    "    mta_tax < 0 or\n",
    "    tip_amount < 0 or\n",
    "    tolls_amount < 0 or\n",
    "    improvement_surcharge < 0 or\n",
    "    total_amount < 0 or\n",
    "    congestion_surcharge < 0 or\n",
    "    Airport_fee < 0\n",
    "\"\"\"\n",
    "check_psdf = filtered_psdf.query(query)\n",
    "check_psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    RatecodeID >= 0 and\n",
    "    PULocationID >= 0 and\n",
    "    DOLocationID >= 0 and\n",
    "    payment_type >= 0 and\n",
    "    fare_amount >= 0 and\n",
    "    extra >= 0 and\n",
    "    mta_tax >= 0 and\n",
    "    tip_amount >= 0 and\n",
    "    tolls_amount >= 0 and\n",
    "    improvement_surcharge >= 0 and\n",
    "    total_amount >= 0 and\n",
    "    congestion_surcharge >= 0 and\n",
    "    Airport_fee >= 0\n",
    "\"\"\"\n",
    "filtered_psdf = filtered_psdf.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>estimated_cost_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, Airport_fee, estimated_cost_amount]\n",
       "Index: []"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 재확인\n",
    "query = f\"\"\"\n",
    "    RatecodeID < 0 or\n",
    "    PULocationID < 0 or\n",
    "    DOLocationID < 0 or\n",
    "    payment_type < 0 or\n",
    "    fare_amount < 0 or\n",
    "    extra < 0 or\n",
    "    mta_tax < 0 or\n",
    "    tip_amount < 0 or\n",
    "    tolls_amount < 0 or\n",
    "    improvement_surcharge < 0 or\n",
    "    total_amount < 0 or\n",
    "    congestion_surcharge < 0 or\n",
    "    Airport_fee < 0\n",
    "\"\"\"\n",
    "check_psdf = filtered_psdf.query(query)\n",
    "check_psdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비현실적인 거리 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/workspace/python/DE/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "[Stage 220:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 5 entries, 0 to 4\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   trip_distance_group  5 non-null      object\n",
      " 1   trip_count           5 non-null      int64 \n",
      "dtypes: int64(1), object(1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# filtered_psdf['trip_distance'].quantile(0.999)\n",
    "filtered_psdf.to_spark().createOrReplaceTempView(\"trips\")\n",
    "\n",
    "# 구간 나눠 SQL로 처리\n",
    "sql = \"\"\"\n",
    "    WITH trip_distance_outliers AS (\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN trip_distance <= 10 THEN \"0-10 miles\"\n",
    "                WHEN trip_distance <= 100 THEN \"10-100 miles\"\n",
    "                WHEN trip_distance <= 1000 THEN \"100-1000 miles\"\n",
    "                WHEN trip_distance <= 10000 THEN \"1000-10000 miles\"\n",
    "                ELSE \"10000+ miles\"\n",
    "            END AS trip_distance_group\n",
    "        FROM trips\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        trip_distance_group,\n",
    "        COUNT(*) AS trip_count\n",
    "    FROM trip_distance_outliers\n",
    "    GROUP BY trip_distance_group        \n",
    "    ORDER BY \n",
    "        CASE trip_distance_group\n",
    "                WHEN \"0-10 miles\" THEN 1\n",
    "                WHEN \"10-100 miles\" THEN 2\n",
    "                WHEN \"100-1000 miles\" THEN 3\n",
    "                WHEN \"1000-10000 miles\" THEN 4\n",
    "                ELSE 5\n",
    "            END DESC\n",
    "\"\"\"\n",
    "\n",
    "result = ps.sql(sql)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2.718641e+06\n",
       "mean     3.217765e+00\n",
       "std      4.284442e+00\n",
       "min      1.000000e-02\n",
       "25%      1.010000e+00\n",
       "50%      1.700000e+00\n",
       "75%      3.100000e+00\n",
       "max      3.825000e+02\n",
       "Name: trip_distance, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_psdf['trip_distance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/workspace/python/DE/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_psdf[filtered_psdf['trip_distance'] > 100].to_pandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_psdf.shape (2718659, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 260:=======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_psdf.shape (2718641, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"filtered_psdf.shape\", filtered_psdf.shape)\n",
    "filtered_psdf = filtered_psdf.query(\n",
    "    \"\"\"\n",
    "    trip_distance <= 100\n",
    "    or trip_distance > 100 and fare_amount > 200\n",
    "    \"\"\"\n",
    ")\n",
    "print(\"filtered_psdf.shape\", filtered_psdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Logic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 296:=======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue: 74430725.46007907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 총 매출\n",
    "tot_revenue = filtered_psdf['total_amount'].sum()\n",
    "print(f\"Total revenue: {tot_revenue}\")\n",
    "\n",
    "# 날짜별 매출\n",
    "filtered_psdf.to_spark().createOrReplaceTempView(\"trips\")\n",
    "sql = \"\"\"\n",
    "    WITH pickup_\n",
    "    SELECT\n",
    "        DATE(tpep_pickup_datetime) AS pickup_date,\n",
    "        total_amount\n",
    "    FROM trips\n",
    "    GROUP BY pickup_date\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips: 2718651\n",
      "Total revenue: 74431279.74007908\n",
      "Average distance: 3.419221959714512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stats = filtered_rdd.aggregate(\n",
    "    (0, 0.0, 0), # 초기값 (trip 수, 총 매출, 평균 거리)\n",
    "    # combine value\n",
    "    # # 각 partition에서 수행할 연산 (map)\n",
    "    lambda acc, x: (\n",
    "        acc[0] +1, \n",
    "        acc[1] + x['total_amount'],\n",
    "        acc[2] + x['trip_distance'],\n",
    "        ),\n",
    "    # 각 partition 결과를 합치는 연산 (combine combiners)\n",
    "    lambda acc1, acc2: ( # \n",
    "        acc1[0] + acc2[0], \n",
    "        acc1[1] + acc2[1],\n",
    "        acc1[2] + acc2[2],        \n",
    "        ),     \n",
    ")\n",
    "\n",
    "print(f\"Total trips: {stats[0]}\")\n",
    "print(f\"Total revenue: {stats[1]}\")\n",
    "print(f\"Average distance: {stats[2] / stats[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01: 102070\n",
      "2024-02-02: 98021\n",
      "2024-02-03: 100990\n",
      "2024-02-04: 80983\n",
      "2024-02-05: 82904\n",
      "2024-02-06: 95328\n",
      "2024-02-07: 95906\n",
      "2024-02-08: 105306\n",
      "2024-02-09: 101060\n",
      "2024-02-10: 102715\n",
      "2024-02-11: 85507\n",
      "2024-02-12: 85609\n",
      "2024-02-13: 64354\n",
      "2024-02-14: 108629\n",
      "2024-02-15: 107069\n",
      "2024-02-16: 94776\n",
      "2024-02-17: 91075\n",
      "2024-02-18: 80718\n",
      "2024-02-19: 70098\n",
      "2024-02-20: 90266\n",
      "2024-02-21: 96718\n",
      "2024-02-22: 105507\n",
      "2024-02-23: 94591\n",
      "2024-02-24: 103426\n",
      "2024-02-25: 82841\n",
      "2024-02-26: 79291\n",
      "2024-02-27: 100885\n",
      "2024-02-28: 100272\n",
      "2024-02-29: 111736\n"
     ]
    }
   ],
   "source": [
    "# 1. map, reduceByKey, sortByKey 사용\n",
    "# number of trips per day\n",
    "daily_trips = filtered_rdd.map(\n",
    "    lambda x: (x['tpep_pickup_datetime'].date(), 1)\n",
    ").reduceByKey(lambda x, y: x + y).sortByKey()\n",
    "\n",
    "for date, count in daily_trips.collect():\n",
    "    print(f\"{date}: {count}\")\n",
    "\n",
    "# 2. groupByKey 사용 (누적하는 것이 아닌 한 번에 메모리에 올리기 때문에 메모리 부족할 수 있음)\n",
    "# 2m 30s\n",
    "# number of trips per day\n",
    "# daily_trips = filtered_rdd.keyBy(\n",
    "#     lambda x: x['tpep_pickup_datetime'].strftime('%Y-%m-%d')\n",
    "# ).groupByKey().mapValues(\n",
    "#     lambda x: len(list(x))\n",
    "# ).collect()\n",
    "\n",
    "# for date, count in daily_trips:\n",
    "#     print(f\"{date}: {count}\")\n",
    "\n",
    "# 3. Join도 가능하지만 Inner 값이기에 비효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01: 2824015.919999942\n",
      "2024-02-02: 2620733.9299999326\n",
      "2024-02-03: 2491385.499999978\n",
      "2024-02-04: 2237400.2499998766\n",
      "2024-02-05: 2316529.6799999434\n",
      "2024-02-06: 2580443.3999999897\n",
      "2024-02-07: 2696189.720000007\n",
      "2024-02-08: 2960078.48999986\n",
      "2024-02-09: 2804011.5299998904\n",
      "2024-02-10: 2608000.2499999367\n",
      "2024-02-11: 2330352.6399999503\n",
      "2024-02-12: 2400751.849999933\n",
      "2024-02-13: 1637187.279999944\n",
      "2024-02-14: 2909888.909999908\n",
      "2024-02-15: 2928848.609999918\n",
      "2024-02-16: 2679060.249999922\n",
      "2024-02-17: 2312374.7899999036\n",
      "2024-02-18: 2187241.4499999024\n",
      "2024-02-19: 2080471.0499999125\n",
      "2024-02-20: 2546112.629999959\n",
      "2024-02-21: 2635096.6199999717\n",
      "2024-02-22: 2952750.1499999287\n",
      "2024-02-23: 2614497.559999943\n",
      "2024-02-24: 2626553.7999999304\n",
      "2024-02-25: 2336178.379999912\n",
      "2024-02-26: 2398380.339999892\n",
      "2024-02-27: 2768085.449999888\n",
      "2024-02-28: 2819418.339999884\n",
      "2024-02-29: 3129240.9699998614\n"
     ]
    }
   ],
   "source": [
    "# Total revenue per day\n",
    "daily_revenue = filtered_rdd.map(\n",
    "    lambda x: (x['tpep_pickup_datetime'].date(), x['total_amount'])\n",
    ").reduceByKey(lambda x, y: x + y).sortByKey()\n",
    "\n",
    "for date, revenue in daily_revenue.collect():\n",
    "    print(f\"{date}: {revenue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2024, 2, 4), 80983, 2237400.2499998766),\n",
       " (datetime.date(2024, 2, 13), 64354, 1637187.279999944),\n",
       " (datetime.date(2024, 2, 17), 91075, 2312374.7899999036),\n",
       " (datetime.date(2024, 2, 21), 96718, 2635096.6199999717),\n",
       " (datetime.date(2024, 2, 1), 102070, 2824015.919999942),\n",
       " (datetime.date(2024, 2, 8), 105306, 2960078.48999986),\n",
       " (datetime.date(2024, 2, 11), 85507, 2330352.6399999503),\n",
       " (datetime.date(2024, 2, 14), 108629, 2909888.909999908),\n",
       " (datetime.date(2024, 2, 16), 94776, 2679060.249999922),\n",
       " (datetime.date(2024, 2, 19), 70098, 2080471.0499999125),\n",
       " (datetime.date(2024, 2, 9), 101060, 2804011.5299998904),\n",
       " (datetime.date(2024, 2, 24), 103426, 2626553.7999999304),\n",
       " (datetime.date(2024, 2, 26), 79291, 2398380.339999892),\n",
       " (datetime.date(2024, 2, 6), 95328, 2580443.3999999897),\n",
       " (datetime.date(2024, 2, 15), 107069, 2928848.609999918),\n",
       " (datetime.date(2024, 2, 28), 100272, 2819418.339999884),\n",
       " (datetime.date(2024, 2, 18), 80718, 2187241.4499999024),\n",
       " (datetime.date(2024, 2, 25), 82841, 2336178.379999912),\n",
       " (datetime.date(2024, 2, 10), 102715, 2608000.2499999367),\n",
       " (datetime.date(2024, 2, 7), 95906, 2696189.720000007),\n",
       " (datetime.date(2024, 2, 20), 90266, 2546112.629999959),\n",
       " (datetime.date(2024, 2, 22), 105507, 2952750.1499999287),\n",
       " (datetime.date(2024, 2, 27), 100885, 2768085.449999888),\n",
       " (datetime.date(2024, 2, 5), 82904, 2316529.6799999434),\n",
       " (datetime.date(2024, 2, 23), 94591, 2614497.559999943),\n",
       " (datetime.date(2024, 2, 3), 100990, 2491385.499999978),\n",
       " (datetime.date(2024, 2, 2), 98021, 2620733.9299999326),\n",
       " (datetime.date(2024, 2, 12), 85609, 2400751.849999933),\n",
       " (datetime.date(2024, 2, 29), 111736, 3129240.9699998614)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_trips_revenue = daily_trips.join(daily_revenue).map(\n",
    "    lambda x: (x[0], x[1][0], x[1][1])\n",
    ")\n",
    "daily_trips_revenue.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'materials/data_engineering_course_materials/missions/W5/results/'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# 기본적으로 partition으로 저장됨\n",
    "\n",
    "# Save results as CSV\n",
    "daily_trips_revenue = daily_trips_revenue.toDF(['date', 'count', 'revenue'])\n",
    "# 파티션 1개로 저장하기\n",
    "daily_trips_revenue.coalesce(1).write.csv(f'{base_dir}/daily_trips_revenues.csv', header=True, mode=\"overwrite\")\n",
    "\n",
    "# Save results as Parquet\n",
    "stats_df = spark.createDataFrame([stats], ['total_trips', 'total_revenue', 'avg_distance'])\n",
    "stats_df.coalesce(1).write.parquet(f'{base_dir}/stats.parquet', mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
