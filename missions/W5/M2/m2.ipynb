{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:47.579924Z",
     "start_time": "2025-02-04T06:43:47.575917Z"
    }
   },
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import *"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:26:43.489891Z",
     "start_time": "2025-02-04T05:26:41.737840Z"
    }
   },
   "source": [
    "spark = SparkSession.builder.appName(\"M2\") \\\n",
    "                            .master(\"local[*]\") \\\n",
    "                            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                            .getOrCreate()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 14:26:42 WARN Utils: Your hostname, sumins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.20.137 instead (on interface en0)\n",
      "25/02/04 14:26:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/04 14:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/04 14:26:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:26:46.652337Z",
     "start_time": "2025-02-04T05:26:46.644644Z"
    }
   },
   "source": [
    "spark.sparkContext"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SparkContext master=local[*] appName=M2>"
      ],
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.20.137:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>M2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:26.872914Z",
     "start_time": "2025-02-04T06:43:26.731799Z"
    }
   },
   "source": [
    "df = spark.read.parquet(\"../data/*.parquet\").dropna()"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:27.331190Z",
     "start_time": "2025-02-04T06:43:27.309457Z"
    }
   },
   "source": [
    "df = df.withColumn('tpep_pickup_datetime', F.col('tpep_pickup_datetime').cast(TimestampType()))\n",
    "df = df.withColumn('tpep_dropoff_datetime', F.col('tpep_dropoff_datetime').cast(TimestampType()))"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:27.968723Z",
     "start_time": "2025-02-04T06:43:27.965127Z"
    }
   },
   "source": [
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:28.526821Z",
     "start_time": "2025-02-04T06:43:28.506849Z"
    }
   },
   "source": [
    "columns_to_check = [\"passenger_count\", \"trip_distance\"]\n",
    "df_filtered = df.filter(reduce(lambda c1, c2: c1 & c2, [F.col(c) > 0 for c in columns_to_check])) \\\n",
    "                .filter(F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\") > 0)"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:32.948721Z",
     "start_time": "2025-02-04T06:43:28.973557Z"
    }
   },
   "source": [
    "high_value = df_filtered.agg(\n",
    "    F.percentile_approx(\"trip_distance\", 0.99, 100).alias(\"percentiles\")\n",
    ").collect()[0][\"percentiles\"]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:41.429284Z",
     "start_time": "2025-02-04T06:43:41.415479Z"
    }
   },
   "source": [
    "df_filtered = df_filtered.filter(df_filtered.trip_distance < high_value)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:42.300468Z",
     "start_time": "2025-02-04T06:43:42.270811Z"
    }
   },
   "source": [
    "weather_schema = StructType([\n",
    "    StructField('Datetime', TimestampType(), True),\n",
    "    StructField('Temperature', IntegerType(), True),\n",
    "    StructField('Humidity', IntegerType(), True),\n",
    "    StructField('Wind Speed', IntegerType(), True),\n",
    "    StructField('Condition', StringType(), True)\n",
    "])\n",
    "weather = spark.read.csv(\"../data/weather.csv\", header=True, schema=weather_schema, enforceSchema=False)\n",
    "weather.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- Temperature: integer (nullable = true)\n",
      " |-- Humidity: integer (nullable = true)\n",
      " |-- Wind Speed: integer (nullable = true)\n",
      " |-- Condition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:43:50.529340Z",
     "start_time": "2025-02-04T06:43:50.456335Z"
    }
   },
   "source": [
    "df_tr_1 = df_filtered.filter(df_filtered.passenger_count >= 1).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_tr_2 = df_tr_1.agg(\n",
    "    F.count('*'),\n",
    "    F.avg(F.col('trip_distance')),\n",
    "    F.sum(F.col('total_amount'))\n",
    "    )\n",
    "df_tr_3 = df_tr_1.join(\n",
    "    weather,\n",
    "    (\n",
    "        F.date_format(df_tr_1['tpep_pickup_datetime'], 'yyyy-MM-dd-HH')\n",
    "        == F.date_format(weather['Datetime'], 'yyyy-MM-dd-HH')\n",
    "     ),\n",
    "    'inner'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:44:11.361185Z",
     "start_time": "2025-02-04T06:43:51.526828Z"
    }
   },
   "source": "df_tr_2.collect()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=32924293, avg(trip_distance)=3.573478439156939, sum(total_amount)=926045447.3687849)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:45:15.707747Z",
     "start_time": "2025-02-04T06:44:12.232944Z"
    }
   },
   "cell_type": "code",
   "source": "df_tr_3.coalesce(1).write.mode(\"overwrite\").parquet(\"joined\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softeer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
