"""
ì´ ì½”ë“œëŠ” Sparkë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ì£¼ìœ¨(Ï€)ì„ ì¶”ì •í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.
100ë§Œ ê°œì˜ ìƒ˜í”Œì„ ìƒì„±í•˜ê³ , ê° ìƒ˜í”Œì´ ì› ì•ˆì— ì†í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ì›ì£¼ìœ¨ì„ ì¶”ì •í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” CSV íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
"""
from pyspark.sql import SparkSession
import os
import random

def inside(_):
    x, y = random.random(), random.random()
    return 1 if x*x + y*y < 1 else 0

if __name__ == "__main__":
    spark = SparkSession.builder.appName("Pi Estimation").getOrCreate()

    n = 1000000  # 100ë§Œ ê°œì˜ ìƒ˜í”Œ
    count = spark.sparkContext.parallelize(range(n)).map(inside).reduce(lambda a, b: a + b)
    pi_estimate = 4.0 * count / n

    # ğŸ”¹ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = "/opt/spark/output"
    os.makedirs(output_dir, exist_ok=True)

    # ğŸ”¹ ê²°ê³¼ë¥¼ Spark DataFrameìœ¼ë¡œ ë³€í™˜
    df = spark.createDataFrame([(pi_estimate,)], ["Estimated Pi Value"])
    
    # # ğŸ”¹ ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥ (coalesce(1) ì‚¬ìš©)
    # df.coalesce(1).write.mode("overwrite").csv(output_dir + "/pi_estimate.csv", header=True)

    # ğŸ”¹ CSV íŒŒì¼ë¡œ ì €ì¥
    csv_output_path = os.path.join(output_dir, "pi_estimate.csv")
    df.write.mode("overwrite").csv(csv_output_path, header=True)

    print(f"Estimated Pi Value: {pi_estimate}")
    print(f"CSV Result saved to {csv_output_path}")

    spark.stop()
    
    
    
"""
ì£¼ì„ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ CSV íŒŒì¼ê³¼ Parquet íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
"""
# from pyspark.sql import SparkSession
# import os
# import random

# def inside(_):
#     x, y = random.random(), random.random()
#     return 1 if x*x + y*y < 1 else 0

# if __name__ == "__main__":
#     spark = SparkSession.builder.appName("Pi Estimation").getOrCreate()

#     n = 1000000  # 100ë§Œ ê°œì˜ ìƒ˜í”Œ
#     count = spark.sparkContext.parallelize(range(n)).map(inside).reduce(lambda a, b: a + b)
#     pi_estimate = 4.0 * count / n

#     # ğŸ”¹ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
#     output_dir = "/opt/spark/output"
#     os.makedirs(output_dir, exist_ok=True)

#     # ğŸ”¹ ê²°ê³¼ë¥¼ Spark DataFrameìœ¼ë¡œ ë³€í™˜
#     df = spark.createDataFrame([(pi_estimate,)], ["Estimated Pi Value"])

#     # ğŸ”¹ CSV íŒŒì¼ë¡œ ì €ì¥
#     csv_output_path = os.path.join(output_dir, "pi_estimate.csv")
#     df.write.mode("overwrite").csv(csv_output_path, header=True)

#     # ğŸ”¹ Parquet íŒŒì¼ë¡œ ì €ì¥
#     parquet_output_path = os.path.join(output_dir, "pi_estimate.parquet")
#     df.write.mode("overwrite").parquet(parquet_output_path)

#     print(f"Estimated Pi Value: {pi_estimate}")
#     print(f"CSV Result saved to {csv_output_path}")
#     print(f"Parquet Result saved to {parquet_output_path}")

#     spark.stop()




"""
ì£¼ì„ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ txt íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
"""
# from pyspark.sql import SparkSession
# import os
# import random

# def inside(_):
#     x, y = random.random(), random.random()
#     return 1 if x*x + y*y < 1 else 0

# if __name__ == "__main__":
#     spark = SparkSession.builder.appName("Pi Estimation").getOrCreate()

#     n = 1000000  # 100ë§Œ ê°œì˜ ìƒ˜í”Œ
#     count = spark.sparkContext.parallelize(range(n)).map(inside).reduce(lambda a, b: a + b)
#     pi_estimate = 4.0 * count / n

#     # ğŸ”¹ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±)
#     output_dir = "/opt/spark/output"
#     os.makedirs(output_dir, exist_ok=True)

#     # ğŸ”¹ ê²°ê³¼ íŒŒì¼ ì €ì¥
#     output_path = os.path.join(output_dir, "pi_estimate.txt")
#     with open(output_path, "w") as f:
#         f.write(f"Estimated Pi Value: {pi_estimate}\n")

#     print(f"Estimated Pi Value: {pi_estimate}")
#     print(f"Result saved to {output_path}")

#     spark.stop()
